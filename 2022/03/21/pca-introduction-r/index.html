<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Principal Component Analysis - Theory and Application with R - Ahsanul&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="google-site-verification" content="iBD1L5iU9O9ELsXiS9NFeW_Qyt7w2phRuHzfOCbCGwY" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Md. Ahsanul Islam" /><meta name="description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..." /><meta name="keywords" content="Ahsanul, statistics, R programming, Bangladesh" />






<meta name="generator" content="Hugo 0.95.0 with theme even" />


<link rel="canonical" href="/2022/03/21/pca-introduction-r/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Principal Component Analysis - Theory and Application with R" />
<meta property="og:description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2022/03/21/pca-introduction-r/" /><meta property="og:image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-03-21T19:20:00+06:00" />
<meta property="article:modified_time" content="2022-03-21T19:20:00+06:00" />

<meta itemprop="name" content="Principal Component Analysis - Theory and Application with R">
<meta itemprop="description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..."><meta itemprop="datePublished" content="2022-03-21T19:20:00+06:00" />
<meta itemprop="dateModified" content="2022-03-21T19:20:00+06:00" />
<meta itemprop="wordCount" content="3859"><meta itemprop="image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png">
<meta itemprop="keywords" content="R,PCA,scree plot,biplot," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png"/>

<meta name="twitter:title" content="Principal Component Analysis - Theory and Application with R"/>
<meta name="twitter:description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Ahsanul&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Posts</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About Me</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Ahsanul&#39;s Blog</a>
</div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7037202679826113"
     crossorigin="anonymous"></script>
 




<script async src="https://www.googletagmanager.com/gtag/js?id=G-XHMN5CZBNV"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XHMN5CZBNV', { 'anonymize_ip': true });
}
</script>
<meta property="og:title" content="Principal Component Analysis - Theory and Application with R" />
<meta property="og:description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2022/03/21/pca-introduction-r/" /><meta property="og:image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-03-21T19:20:00+06:00" />
<meta property="article:modified_time" content="2022-03-21T19:20:00+06:00" />

<meta itemprop="name" content="Principal Component Analysis - Theory and Application with R">
<meta itemprop="description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..."><meta itemprop="datePublished" content="2022-03-21T19:20:00+06:00" />
<meta itemprop="dateModified" content="2022-03-21T19:20:00+06:00" />
<meta itemprop="wordCount" content="3859"><meta itemprop="image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png">
<meta itemprop="keywords" content="R,PCA,scree plot,biplot," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/PCA-preview.png"/>

<meta name="twitter:title" content="Principal Component Analysis - Theory and Application with R"/>
<meta name="twitter:description" content="PCA is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information is preserved by those linear combinations. Before diving into the PCA, there are a few things one should be aware of..."/>
<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Posts</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About Me</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Principal Component Analysis - Theory and Application with R</h1>

      <div class="post-meta">
        <span class="post-time"> Mar 21, 2022 </span>
        <div class="post-category">
            <a href="/categories/multivariate-analysis/"> Multivariate Analysis </a>
            </div>
          <span class="more-meta"> 3859 words </span>
          <span class="more-meta"> 19 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    
  </div>
</div>
    <div class="post-content">
      
<script src="/2022/03/21/pca-introduction-r/index.en_files/header-attrs/header-attrs.js"></script>
<script src="/2022/03/21/pca-introduction-r/index.en_files/kePrint/kePrint.js"></script>
<link href="/2022/03/21/pca-introduction-r/index.en_files/lightable/lightable.css" rel="stylesheet" />
<script src="/2022/03/21/pca-introduction-r/index.en_files/htmlwidgets/htmlwidgets.js"></script>
<script src="/2022/03/21/pca-introduction-r/index.en_files/plotly-binding/plotly.js"></script>
<script src="/2022/03/21/pca-introduction-r/index.en_files/typedarray/typedarray.min.js"></script>
<script src="/2022/03/21/pca-introduction-r/index.en_files/jquery/jquery.min.js"></script>
<link href="/2022/03/21/pca-introduction-r/index.en_files/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
<script src="/2022/03/21/pca-introduction-r/index.en_files/crosstalk/js/crosstalk.min.js"></script>
<link href="/2022/03/21/pca-introduction-r/index.en_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/2022/03/21/pca-introduction-r/index.en_files/plotly-main/plotly-latest.min.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#underlying-theory" id="toc-underlying-theory">Underlying Theory</a>
<ul>
<li><a href="#pre-requisites" id="toc-pre-requisites">Pre-requisites</a></li>
<li><a href="#theory" id="toc-theory">Theory</a>
<ul>
<li><a href="#preserving-variance" id="toc-preserving-variance">Preserving Variance</a></li>
<li><a href="#measuring-accounted-variance-by-pcs" id="toc-measuring-accounted-variance-by-pcs">Measuring Accounted Variance by PCs</a></li>
</ul></li>
</ul></li>
<li><a href="#practical-demonstration" id="toc-practical-demonstration">Practical Demonstration</a>
<ul>
<li><a href="#dataset" id="toc-dataset">Dataset</a></li>
<li><a href="#manual-calculation" id="toc-manual-calculation">Manual Calculation</a>
<ul>
<li><a href="#using-correlation-matrix" id="toc-using-correlation-matrix">Using Correlation Matrix</a></li>
<li><a href="#using-variance-covariance-matrix" id="toc-using-variance-covariance-matrix">Using Variance-Covariance Matrix</a></li>
</ul></li>
<li><a href="#using-prcomp" id="toc-using-prcomp">Using prcomp()</a>
<ul>
<li><a href="#individual-points-plot" id="toc-individual-points-plot">Individual Points Plot</a></li>
<li><a href="#d-plot" id="toc-d-plot">3D Plot</a></li>
</ul></li>
<li><a href="#scree-plot" id="toc-scree-plot">Scree plot</a></li>
<li><a href="#factominers-pca" id="toc-factominers-pca">FactoMineR’s PCA()</a></li>
<li><a href="#biplot" id="toc-biplot">Biplot</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<hr />
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Karl Pearson invented PCA in 1901, defining it as <a href="https://www.tandfonline.com/doi/abs/10.1080/14786440109462720">“On lines and planes of closest fit to systems of points in space.”</a> Harold Hotelling later developed and named it independently in the <a href="https://content.apa.org/record/1934-00645-001">1930s</a>.</p>
<p>Principal component analysis (PCA) is a technique to transform the original set of variables into a smaller set of linear combination of variables so that most of the statistical information (variance) in the original data is preserved by those linear combinations. It accomplishes this by generating new uncorrelated variables that gradually maximize variance. Finding such new variables, known as principal components (PCs), is essentially the same as solving an eigenvalue/eigenvector problem.<sup><a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202">1</a></sup></p>
<p>An eigen vector is a direction of the transformation of the original dataset for which no information in the data is lost and an eigenvalue shows the amount of variance that is explained in the data in that direction. So the eigen vector corresponding to the highest eigenvalue is the first principal component.</p>
<p>Geometrically to say, PCA is a type of linear transformation of a given dataset <span class="math inline">\(X\)</span> that contains values for a given number of variables (or say coordinates) for a given number of spaces. This linear transformation fits this dataset to a new coordinate system in such a way that the first coordinate (component) has the most variation, and each subsequent coordinate is orthogonal to the first and has less variance.<sup><a href="https://www.datacamp.com/community/tutorials/pca-analysis-r">2</a>,<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579">3</a></sup></p>
<hr />
</div>
<div id="underlying-theory" class="section level1">
<h1>Underlying Theory</h1>
<p><img src="image/pca1.gif" /></p>
<div id="pre-requisites" class="section level2">
<h2>Pre-requisites</h2>
<p>Before diving into the PCA, there are a few things one should be aware of -</p>
<ol style="list-style-type: decimal">
<li>Linear Algebra
<ul>
<li>Trace of a matrix and its <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Properties">properties</a></li>
<li>Determinant of a matrix</li>
<li><a href="https://www.youtube.com/watch?v=WTLl03D4TNA">Diagonalization of a matrix</a></li>
<li><a href="https://math.stackexchange.com/questions/2190841/what-is-orthogonal-transformation">Orthogonal transformation</a></li>
<li><a href="https://youtu.be/PFDu9oVAE-g">Eigen values and eigen vectors</a></li>
</ul></li>
<li>Statistics
<ul>
<li>Mean vector</li>
<li>Variance-Covariance Matrix</li>
<li>Correlation Matrix</li>
</ul></li>
</ol>
</div>
<div id="theory" class="section level2">
<h2>Theory</h2>
<p>Let, p be the number of variables, n be the number of observations, <span class="math inline">\(X\)</span> be a <span class="math inline">\((p\ \times \ n)\)</span> matrix of a dataset, mean vector <span class="math inline">\(\bar{X}\)</span> be a zero vector and the variance-covariance matrix <span class="math inline">\(\Sigma\)</span> be a real <a href="https://nhigham.com/2020/07/21/what-is-a-symmetric-positive-definite-matrix/">positive definite matrix</a> or at least <a href="https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf">positive semi-definite</a> of rank <span class="math inline">\(r \le p\)</span>.</p>
<p>Let, <span class="math inline">\(\lambda_i\ \forall\ i=1,2,...,p\)</span> be the eigen values of <span class="math inline">\(\Sigma\)</span> where, <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \lambda_3 &gt; ... &gt; \lambda_p\)</span> and their corresponding eigenvectors are <span class="math inline">\(\gamma_1\)</span>, <span class="math inline">\(\gamma_2\)</span>, <span class="math inline">\(\gamma_3\)</span>, …, <span class="math inline">\(\gamma_p\)</span>.</p>
<p>where, <span class="math inline">\(D\)</span> is a diagonal matrix where the diagonal elements are the eigenvalues -
<span class="math display">\[
D =\begin{bmatrix}
\lambda_1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; \lambda_2 &amp; ... &amp; 0\\
. &amp; . &amp; ... &amp; .\\
0 &amp; 0 &amp; ... &amp; \lambda_p
\end{bmatrix}_{p\ \times\ p}
\]</span></p>
<p>and the matrix of eigen vectors,
<span class="math display">\[
\Gamma =\begin{bmatrix} \gamma_1,\ \gamma_2,\ ...\ ,\gamma_p\\ \end{bmatrix}_{p\ \times\ p}
\]</span></p>
<p>The matrix <span class="math inline">\(\Gamma\)</span> diagonalizes <span class="math inline">\(\Sigma\)</span> such that, <span class="math inline">\(D = \Gamma&#39; \Sigma \Gamma\)</span> or, <span class="math inline">\(\Sigma=\Gamma D\Gamma&#39;\)</span></p>
<p><em>Since <span class="math inline">\(\Gamma\)</span> is an orthogonal matrix, then <span class="math inline">\(\Gamma\Gamma&#39;=\Gamma&#39;\Gamma = I\)</span>. </em> <a href="https://math.stackexchange.com/questions/142645/are-all-eigenvectors-of-any-matrix-always-orthogonal">link</a></p>
<p>We consider an orthogonal transformation (that will preserve all the information) of <span class="math inline">\(X\)</span> into <span class="math inline">\(Y\)</span> by - <span class="math inline">\(Y=\Gamma&#39;X\)</span><br />
<span class="math inline">\(or,\begin{bmatrix}Y_1\\Y_2\\.\\Y_p\end{bmatrix}=\Gamma&#39;\begin{bmatrix}X_1\\X_2\\.\\X_p\end{bmatrix}\)</span></p>
<p>Here, <span class="math inline">\(Y_1\)</span>, <span class="math inline">\(Y_2\)</span>, …, <span class="math inline">\(Y_p\)</span> are the p components of <span class="math inline">\(Y\)</span> and are called the Principal Components.</p>
<p>For example if <span class="math inline">\(X=[x_1\ \ \ x_2]\)</span> is a two variable data set and the first eigen vactor is <span class="math inline">\([a\ \ \  b]&#39;\)</span>.</p>
<p>Then the first principal component will be - <span class="math inline">\(P_1=x_1\times a+x_2 \times b\)</span></p>
<p>Similarly if the second eigen vector is <span class="math inline">\([c\ \ \ d]&#39;\)</span> then the second principal component will be - <span class="math inline">\(P_2=x_1\times c+x_2 \times d\)</span></p>
<hr />
<div id="preserving-variance" class="section level3">
<h3>Preserving Variance</h3>
<p><strong>How does the technique preserve the total variation after the transformation?</strong><br />
</p>
<p>Since <span class="math inline">\(\Sigma\)</span> is the variance-covariance matrix of <span class="math inline">\(X\)</span>, we can measure the following things:</p>
<ol style="list-style-type: decimal">
<li>Overall measure of the variability of <span class="math inline">\(X\)</span> as <span class="math inline">\(trace(\Sigma)\)</span> or,<br />
</li>
<li>Generalized variance of <span class="math inline">\(X\)</span> as <span class="math inline">\(|\Sigma|\)</span></li>
</ol>
<p>Now, <span class="math inline">\(trace(\Sigma)\)</span><br />
<span class="math inline">\(=trace(\Gamma D\Gamma&#39;)\)</span><br />
<span class="math inline">\(=trace(\Gamma&#39;\Gamma D)\)</span> <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Cyclic_property">[Cyclic property]</a><br />
<span class="math inline">\(=trace(D)\)</span> [Since, <span class="math inline">\(\Gamma\)</span> is an orthogonal matrix]<br />
<span class="math inline">\(=\sum_{i=1}^{p}\lambda_i\)</span></p>
<p>Again, <span class="math inline">\(|\Sigma|\)</span><br />
<span class="math inline">\(=|\Sigma D\Sigma&#39;|\)</span><br />
<span class="math inline">\(=|\Sigma|\times| D|\times|\Sigma&#39;|\)</span><br />
<span class="math inline">\(=|D|\)</span><br />
<span class="math inline">\(=\prod_{i=1}^{p}\lambda_i\)</span></p>
<p>Now we have the transformation, <span class="math inline">\(Y=\Gamma&#39;X\)</span></p>
<p>Here, <span class="math inline">\(E[Y] = E[\Gamma&#39;X]=\Gamma&#39;E[X]=0\)</span><br />
and <span class="math inline">\(V(Y)=E[YY&#39;]=E[\Gamma&#39;XX&#39;\Gamma]=\Gamma&#39;E[XX&#39;]\Gamma=\Gamma&#39; \Sigma \Gamma=D\)</span></p>
<p>Thus, for <span class="math inline">\(Y\)</span>, overall measure of variability is <span class="math inline">\(trace(D)=\sum_{i=1}^{p}\lambda_i\)</span><br />
or, generalized variance is <span class="math inline">\(|D|=\prod_{i=1}^{p}\lambda_i\)</span></p>
<p>Which shows that the total variation of <span class="math inline">\(X\)</span> remains unchanged in <span class="math inline">\(Y\)</span>.</p>
<hr />
</div>
<div id="measuring-accounted-variance-by-pcs" class="section level3">
<h3>Measuring Accounted Variance by PCs</h3>
<p><strong>How to know the percentage of variation that is explained by a principal component?</strong></p>
<p>Since <span class="math inline">\(\Sigma\)</span> is a positive definite matrix, the strict positivity of <span class="math inline">\(\lambda_i\)</span> is guaranteed.</p>
<p>Now, <span class="math inline">\(Y_1\)</span> is the first principal component corresponding to first eigenvalue <span class="math inline">\(\lambda_1\)</span>, and similarly <span class="math inline">\(Y_i\)</span> is the i-th principal component corresponding to i-th eigenvalue <span class="math inline">\(\lambda_i\)</span>.</p>
<p>The percentage of variation of the original data <span class="math inline">\(X\)</span> explained by the i-th principal component is calculated using - <span class="math inline">\(\frac{\lambda_i}{\sum_{i=1}^{p}\lambda_i}\times 100\)</span></p>
<p>At the beginning of the theory development since <span class="math inline">\(\lambda_i\)</span>s were taken such as <span class="math inline">\(\lambda_1\)</span> &gt; <span class="math inline">\(\lambda_2\)</span> &gt; <span class="math inline">\(\lambda_3\)</span> &gt; … &gt;<span class="math inline">\(\lambda_p\)</span>, the first principal component explains the largest amount of variation.</p>
<hr />
</div>
</div>
</div>
<div id="practical-demonstration" class="section level1">
<h1>Practical Demonstration</h1>
<div id="dataset" class="section level2">
<h2>Dataset</h2>
<p>Importing a dataset -</p>
<pre class="r"><code>data &lt;- readxl::read_excel(&quot;data/1.2 data.xlsx&quot;)
data$Family &lt;- ifelse(data$Family==0, &quot;Serranidae&quot;,
                      ifelse(data$Family==1, &quot;Carangidae&quot;, &quot;Sparidae&quot;))
df_unstd &lt;- data[,-8]</code></pre>
<p><a href="https://md-ahsanul.github.io/2022/03/21/pca-introduction-r/data/1.2%20data.xlsx">Download the dataset</a></p>
<p>Data description:</p>
<ol style="list-style-type: decimal">
<li>x1: centrum’s length<br />
</li>
<li>x2: anterior centrum’s width<br />
</li>
<li>x3: posterior centrum’s width<br />
</li>
<li>x4: centrum’s height<br />
</li>
<li>x5: distance between the base of the neural spine and the posterior face of the centrum<br />
</li>
<li>x6: distance between the base of the haemal spine and the posterior face of the centrum<br />
</li>
<li>x7: length of anterior zygapophysis along with other variables<br />
</li>
<li>Family: family of fish specimens; 0 - Serranidae, 1 - Carangidae, 2 - Sparidae</li>
</ol>
<p><a href="http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2">Correlation plot</a> of the original dataset and the principal component transformed (we’ll see it later) dataset -</p>
<pre class="r"><code>library(ggcorrplot)
pc_prcomp &lt;- prcomp(df_unstd, center = TRUE, scale. = TRUE)
corr &lt;- cor(df_unstd)
a &lt;- ggcorrplot(corr, show.legend = F, 
           hc.order = F, type = &quot;upper&quot;, lab = T, 
           digits = 2, method = &quot;square&quot;,
           colors = c(&quot;#6D9EC1&quot;, &quot;white&quot;, &quot;#E46726&quot;)) +
  labs(title = &quot;Correlations Matrix: Original dataset&quot;)
b &lt;- ggcorrplot(cor(pc_prcomp$x), show.legend = F, 
           hc.order = F, type = &quot;upper&quot;, lab = T, 
           digits = 2, method = &quot;square&quot;,
           colors = c(&quot;#6D9EC1&quot;, &quot;white&quot;, &quot;#E46726&quot;)) +
  labs(title = &quot;Correlations Matrix: Principal Component Transformed&quot;)
ggpubr::ggarrange(a, b, 
          ncol = 2, nrow = 1)</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-4-1.png" width="960" /></p>
<p>The idea is that if many variables correlate with one another, they will all contribute strongly to the same principal component.<sup><a href="https://www.datacamp.com/community/tutorials/pca-analysis-r">2</a></sup></p>
</div>
<div id="manual-calculation" class="section level2">
<h2>Manual Calculation</h2>
<p>The principal components can be found using things -</p>
<ol style="list-style-type: decimal">
<li>Variance-covariance matrix<br />
</li>
<li>Correlation matrix</li>
</ol>
<p>If the data is not standardized, it is suggested to use the correlation matrix in principal component analysis. Because the variance of variable is affected by the scale of measurement.</p>
<p>But if the data is standardized, both of variance-covariance matrix and correlation matrix gives the same principal components. So, it is recommended to use standardize the data before performing principal component analysis.</p>
<p>Standardization is done using the following formula -
<span class="math display">\[
Z = \frac{X-\mu}{\sigma}
\]</span></p>
<p>Calculating mean vector -</p>
<pre class="r"><code>mn &lt;- colMeans(df_unstd)   # Mean vector
mn</code></pre>
<pre><code>#       x1       x2       x3       x4       x5       x6       x7 
# 7.297222 5.811111 5.672222 5.672222 2.552778 3.677778 2.369444</code></pre>
<p>Variance-covariance matrix -</p>
<pre class="r"><code>var(df_unstd)</code></pre>
<pre><code>#           x1          x2         x3         x4          x5         x6        x7
# x1 1.3442778  0.90374603 0.93420635  0.9853492  0.35529365 0.52679365 0.3961984
# x2 0.9037460  1.28387302 1.17974603  1.4508889 -0.09203175 0.57996825 0.1774921
# x3 0.9342063  1.17974603 1.35520635  1.5217778  0.06007937 0.58936508 0.2674127
# x4 0.9853492  1.45088889 1.52177778  2.0803492 -0.11277778 0.79450794 0.2434127
# x5 0.3552937 -0.09203175 0.06007937 -0.1127778  0.45627778 0.04749206 0.2662302
# x6 0.5267937  0.57996825 0.58936508  0.7945079  0.04749206 0.38234921 0.1581587
# x7 0.3961984  0.17749206 0.26741270  0.2434127  0.26623016 0.15815873 0.2718968</code></pre>
<p>Extracting the standard deviations -</p>
<pre class="r"><code>vardf_unstd &lt;- diag(var(df_unstd))^0.5
vardf_unstd</code></pre>
<pre><code>#        x1        x2        x3        x4        x5        x6        x7 
# 1.1594299 1.1330812 1.1641333 1.4423416 0.6754834 0.6183439 0.5214373</code></pre>
<p>Subtracting the columns by their corresponding means and dividing by their corresponding standard deviations to obtain standardized dataset -</p>
<pre class="r"><code>df_std &lt;- as.matrix((df_unstd - rep(mn,each = nrow(df_unstd)))/rep(vardf_unstd, each = nrow(df_unstd))) </code></pre>
<p>Mean vector of the new dataset -</p>
<pre class="r"><code>round(colMeans(df_std))</code></pre>
<pre><code># x1 x2 x3 x4 x5 x6 x7 
#  0  0  0  0  0  0  0</code></pre>
<div id="using-correlation-matrix" class="section level3">
<h3>Using Correlation Matrix</h3>
<p>Using correlation matrix of <strong>standardized</strong> data to calculate eigenvalues -</p>
<pre class="r"><code>eigcorr &lt;- eigen(cor(df_std))      # using correlation matrix on standardized data
output &lt;- cbind(eigenvalue = round(eigcorr$values,2),
      cumul_percentage = round(cumsum(eigcorr$values/sum(eigcorr$values))*100, 2))</code></pre>
<table class="table table-striped table-hover" style="font-size: 12px; width: auto !important; ">
<thead>
<tr>
<th style="text-align:right;">
Eigen Values
</th>
<th style="text-align:right;">
Cumulative Percentage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
4.50
</td>
<td style="text-align:right;">
64.32
</td>
</tr>
<tr>
<td style="text-align:right;">
1.77
</td>
<td style="text-align:right;">
89.66
</td>
</tr>
<tr>
<td style="text-align:right;">
0.28
</td>
<td style="text-align:right;">
93.71
</td>
</tr>
<tr>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
96.47
</td>
</tr>
<tr>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
98.56
</td>
</tr>
<tr>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
99.50
</td>
</tr>
<tr>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
100.00
</td>
</tr>
</tbody>
</table>
<p>It is seen that the first principal component can explain 64.32% of the total variation in the data. The second principal component explains 25.35% variance. Similarly the third PC explains 4.04% variance.</p>
<p>Thus the first two principal components together explain 89.66% of the total variance.</p>
<p>Using correlation matrix of <strong>unstandardized</strong> data to calculate eigenvalues -</p>
<pre class="r"><code>eigcorr &lt;- eigen(cor(df_unstd))      # using correlation matrix on unstandardized data
output &lt;- cbind(eigenvalue = round(eigcorr$values,2),
                cumul_percentage = round(cumsum(eigcorr$values/sum(eigcorr$values))*100, 2))</code></pre>
<table class="table table-striped table-hover" style="font-size: 12px; width: auto !important; ">
<thead>
<tr>
<th style="text-align:right;">
Eigen Values
</th>
<th style="text-align:right;">
Cumulative Percentage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
4.50
</td>
<td style="text-align:right;">
64.32
</td>
</tr>
<tr>
<td style="text-align:right;">
1.77
</td>
<td style="text-align:right;">
89.66
</td>
</tr>
<tr>
<td style="text-align:right;">
0.28
</td>
<td style="text-align:right;">
93.71
</td>
</tr>
<tr>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
96.47
</td>
</tr>
<tr>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
98.56
</td>
</tr>
<tr>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
99.50
</td>
</tr>
<tr>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
100.00
</td>
</tr>
</tbody>
</table>
<p>So, it is verified that the principal components are not affected by the scales if the correlation matrix is used.</p>
<p>The eigen vectors -</p>
<pre class="r"><code>round(eigcorr$vectors,3)</code></pre>
<pre><code>#        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]
# [1,] -0.402  0.223  0.778  0.114  0.097  0.398 -0.055
# [2,] -0.423 -0.263  0.218 -0.253  0.268 -0.726 -0.202
# [3,] -0.438 -0.137 -0.157 -0.619 -0.225  0.246  0.519
# [4,] -0.423 -0.271 -0.352  0.071 -0.176  0.340 -0.688
# [5,] -0.110  0.706 -0.027 -0.158 -0.574 -0.297 -0.213
# [6,] -0.439 -0.084 -0.139  0.713 -0.251 -0.210  0.407
# [7,] -0.291  0.532 -0.423  0.027  0.667  0.072  0.053</code></pre>
<p>So the principal component transformation of the original data will be -</p>
<pre class="r"><code>princr_df &lt;- df_std %*% eigcorr$vectors
# View the principal components
scroll_box(height = &quot;300px&quot;, 
           kable_styling(
             kable(
               princr_df, digits = 2, format = &quot;html&quot;, col.names = paste0(&quot;PC&quot;, 1:7)
             ),
             bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;),
             full_width = T, font_size = 12, position = &quot;left&quot;))</code></pre>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; ">
<table class="table table-striped table-hover" style="font-size: 12px; ">
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC1
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC2
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC3
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC4
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC5
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC6
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC7
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-1.66
</td>
<td style="text-align:right;">
-1.61
</td>
<td style="text-align:right;">
-0.45
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
0.32
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:right;">
-0.13
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.56
</td>
<td style="text-align:right;">
-1.88
</td>
<td style="text-align:right;">
-0.55
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
-0.19
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.67
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
0.05
</td>
</tr>
<tr>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
-2.57
</td>
<td style="text-align:right;">
-0.97
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.14
</td>
</tr>
<tr>
<td style="text-align:right;">
1.80
</td>
<td style="text-align:right;">
-2.71
</td>
<td style="text-align:right;">
-0.56
</td>
<td style="text-align:right;">
-0.45
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
-0.01
</td>
<td style="text-align:right;">
-0.40
</td>
</tr>
<tr>
<td style="text-align:right;">
0.53
</td>
<td style="text-align:right;">
-1.85
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
-0.28
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.49
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
-0.38
</td>
<td style="text-align:right;">
0.28
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.24
</td>
<td style="text-align:right;">
-1.94
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
-0.40
</td>
<td style="text-align:right;">
-0.54
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
-1.49
</td>
<td style="text-align:right;">
0.42
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.28
</td>
<td style="text-align:right;">
-0.12
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.32
</td>
<td style="text-align:right;">
-1.82
</td>
<td style="text-align:right;">
1.18
</td>
<td style="text-align:right;">
-0.24
</td>
<td style="text-align:right;">
-0.29
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.10
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.21
</td>
<td style="text-align:right;">
-1.38
</td>
<td style="text-align:right;">
0.26
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
-0.03
</td>
<td style="text-align:right;">
-0.21
</td>
<td style="text-align:right;">
0.31
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.88
</td>
<td style="text-align:right;">
-0.99
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.28
</td>
<td style="text-align:right;">
1.26
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.48
</td>
<td style="text-align:right;">
-0.17
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.16
</td>
<td style="text-align:right;">
1.30
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
-0.51
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.41
</td>
<td style="text-align:right;">
1.23
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
-0.60
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
-0.33
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.87
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
-0.53
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
-0.72
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.48
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.30
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.44
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
-0.11
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.97
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
-0.61
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.05
</td>
<td style="text-align:right;">
-0.44
</td>
<td style="text-align:right;">
-0.29
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
-0.67
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.20
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.84
</td>
<td style="text-align:right;">
1.42
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
0.96
</td>
<td style="text-align:right;">
-0.37
</td>
<td style="text-align:right;">
-0.46
</td>
<td style="text-align:right;">
0.17
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.75
</td>
<td style="text-align:right;">
1.89
</td>
<td style="text-align:right;">
-0.45
</td>
<td style="text-align:right;">
-0.30
</td>
<td style="text-align:right;">
-0.31
</td>
<td style="text-align:right;">
0.23
</td>
<td style="text-align:right;">
-0.20
</td>
</tr>
<tr>
<td style="text-align:right;">
0.87
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
-0.38
</td>
<td style="text-align:right;">
-0.93
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
-0.03
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.44
</td>
<td style="text-align:right;">
2.32
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
-0.19
</td>
<td style="text-align:right;">
0.74
</td>
<td style="text-align:right;">
-0.19
</td>
<td style="text-align:right;">
-0.09
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.87
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
-0.80
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
2.97
</td>
<td style="text-align:right;">
0.26
</td>
<td style="text-align:right;">
-0.22
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
-0.11
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
-0.08
</td>
</tr>
<tr>
<td style="text-align:right;">
2.77
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
-0.60
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
2.23
</td>
<td style="text-align:right;">
0.77
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
-0.37
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
0.10
</td>
</tr>
<tr>
<td style="text-align:right;">
1.64
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
0.89
</td>
<td style="text-align:right;">
-0.25
</td>
<td style="text-align:right;">
-0.56
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
-0.08
</td>
</tr>
<tr>
<td style="text-align:right;">
1.12
</td>
<td style="text-align:right;">
1.05
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
-0.01
</td>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
0.21
</td>
</tr>
<tr>
<td style="text-align:right;">
3.97
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
-0.30
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
-0.24
</td>
</tr>
<tr>
<td style="text-align:right;">
3.21
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
0.17
</td>
<td style="text-align:right;">
-0.03
</td>
<td style="text-align:right;">
-0.06
</td>
<td style="text-align:right;">
-0.25
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
1.69
</td>
<td style="text-align:right;">
0.86
</td>
<td style="text-align:right;">
0.42
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
-0.17
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
-0.05
</td>
</tr>
<tr>
<td style="text-align:right;">
2.21
</td>
<td style="text-align:right;">
0.85
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
2.03
</td>
<td style="text-align:right;">
1.03
</td>
<td style="text-align:right;">
-0.21
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.32
</td>
<td style="text-align:right;">
-0.17
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
3.39
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.26
</td>
</tr>
<tr>
<td style="text-align:right;">
2.71
</td>
<td style="text-align:right;">
0.61
</td>
<td style="text-align:right;">
-0.03
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="using-variance-covariance-matrix" class="section level3">
<h3>Using Variance-Covariance Matrix</h3>
<p>Using variance-covariance matrix of <strong>standardized</strong> data to calculate eigenvalues -</p>
<pre class="r"><code>eigcov &lt;- eigen(cov(df_std))    # using variance-covariance matrix on standardized data
output &lt;- cbind(eigenvalue = round(eigcov$values,2),
                cumul_percentage = round(cumsum(eigcov$values/sum(eigcov$values))*100,2))</code></pre>
<table class="table table-striped table-hover" style="font-size: 12px; width: auto !important; ">
<thead>
<tr>
<th style="text-align:right;">
Eigen Values
</th>
<th style="text-align:right;">
Cumulative Percentage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
4.50
</td>
<td style="text-align:right;">
64.32
</td>
</tr>
<tr>
<td style="text-align:right;">
1.77
</td>
<td style="text-align:right;">
89.66
</td>
</tr>
<tr>
<td style="text-align:right;">
0.28
</td>
<td style="text-align:right;">
93.71
</td>
</tr>
<tr>
<td style="text-align:right;">
0.19
</td>
<td style="text-align:right;">
96.47
</td>
</tr>
<tr>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
98.56
</td>
</tr>
<tr>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
99.50
</td>
</tr>
<tr>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
100.00
</td>
</tr>
</tbody>
</table>
<p>Using variance-covariance matrix of <strong>unstandardized</strong> data to calculate eigenvalues -</p>
<pre class="r"><code>eigcov &lt;- eigen(cov(df_unstd))    # using variance-covariance matrix on unstandardized data
output &lt;- cbind(eigenvalue = round(eigcov$values,2),
                cumul_percentage = round(cumsum(eigcov$values/sum(eigcov$values))*100,2))</code></pre>
<table class="table table-striped table-hover" style="font-size: 12px; width: auto !important; ">
<thead>
<tr>
<th style="text-align:right;">
Eigen Values
</th>
<th style="text-align:right;">
Cumulative Percentage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5.48
</td>
<td style="text-align:right;">
76.39
</td>
</tr>
<tr>
<td style="text-align:right;">
1.08
</td>
<td style="text-align:right;">
91.40
</td>
</tr>
<tr>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
95.46
</td>
</tr>
<tr>
<td style="text-align:right;">
0.16
</td>
<td style="text-align:right;">
97.75
</td>
</tr>
<tr>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
98.93
</td>
</tr>
<tr>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
99.56
</td>
</tr>
<tr>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
100.00
</td>
</tr>
</tbody>
</table>
<p>So the principal components are influenced by the scales. Hence, it is recommended to standardize the data before calculating the principal components.</p>
</div>
</div>
<div id="using-prcomp" class="section level2">
<h2>Using prcomp()</h2>
<p>To make things easier in R, there are two built-in functions to perform PCA - <code>prcomp()</code> and <code>princomp()</code>. The function <code>princomp()</code> uses the spectral decomposition approach while the functions <code>prcomp()</code> and <code>PCA()</code>[from package FactoMineR] use the singular value decomposition (SVD).</p>
<p><em>Note that are two general methods to perform PCA in R:<sup><a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/#:~:text=The%20function%20princomp()%20uses,preferred%20compared%20to%20princomp().">4</a></sup></em></p>
<p><em>1. Spectral decomposition which examines the covariances / correlations between variables</em><br />
<em>2. Singular value decomposition which examines the covariances / correlations between individuals </em></p>
<p>Due to higher level of accuracy <code>prcomp()</code> is preferred over <code>princomp()</code>. <a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/">Standardization</a> can be done by passing two arguments to the <code>prcomp()</code> function <code>center = TRUE</code> and <code>scale. = TRUE</code> -</p>
<pre class="r"><code>pc_prcomp &lt;- prcomp(df_unstd, center = TRUE, scale. = TRUE)
summary(pc_prcomp)</code></pre>
<pre><code># Importance of components:
#                           PC1    PC2     PC3     PC4     PC5     PC6     PC7
# Standard deviation     2.1218 1.3320 0.53204 0.43966 0.38258 0.25684 0.18685
# Proportion of Variance 0.6432 0.2535 0.04044 0.02761 0.02091 0.00942 0.00499
# Cumulative Proportion  0.6432 0.8966 0.93706 0.96468 0.98559 0.99501 1.00000</code></pre>
<p>The result is consistent with the manual calculation.</p>
<p>The function <code>prcomp()</code> returns the following things -</p>
<pre class="r"><code>names(pc_prcomp)</code></pre>
<pre><code># [1] &quot;sdev&quot;     &quot;rotation&quot; &quot;center&quot;   &quot;scale&quot;    &quot;x&quot;</code></pre>
<p><code>x</code> denotes the transformed dataset/principal components -</p>
<pre class="r"><code>pc_prcomp$x</code></pre>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:300px; ">
<table class="table table-striped table-hover" style="font-size: 12px; ">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-23">Table 1: </span>Principal Components
</caption>
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC1
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC2
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC3
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC4
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC5
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC6
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
PC7
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-1.66
</td>
<td style="text-align:right;">
-1.61
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
-0.32
</td>
<td style="text-align:right;">
0.34
</td>
<td style="text-align:right;">
-0.13
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.56
</td>
<td style="text-align:right;">
-1.88
</td>
<td style="text-align:right;">
0.55
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
-0.19
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.67
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
-0.05
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
-0.22
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
0.05
</td>
</tr>
<tr>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
-2.57
</td>
<td style="text-align:right;">
0.97
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.14
</td>
</tr>
<tr>
<td style="text-align:right;">
1.80
</td>
<td style="text-align:right;">
-2.71
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
-0.45
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.01
</td>
<td style="text-align:right;">
-0.40
</td>
</tr>
<tr>
<td style="text-align:right;">
0.53
</td>
<td style="text-align:right;">
-1.85
</td>
<td style="text-align:right;">
-0.51
</td>
<td style="text-align:right;">
-0.28
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.49
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
-0.16
</td>
<td style="text-align:right;">
-0.38
</td>
<td style="text-align:right;">
0.28
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.24
</td>
<td style="text-align:right;">
-1.94
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
0.40
</td>
<td style="text-align:right;">
-0.54
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
-1.49
</td>
<td style="text-align:right;">
-0.42
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.27
</td>
<td style="text-align:right;">
-0.28
</td>
<td style="text-align:right;">
-0.12
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.32
</td>
<td style="text-align:right;">
-1.82
</td>
<td style="text-align:right;">
-1.18
</td>
<td style="text-align:right;">
-0.24
</td>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.10
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.21
</td>
<td style="text-align:right;">
-1.38
</td>
<td style="text-align:right;">
-0.26
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
-0.21
</td>
<td style="text-align:right;">
0.31
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.88
</td>
<td style="text-align:right;">
-0.99
</td>
<td style="text-align:right;">
-0.90
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
-0.20
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.28
</td>
<td style="text-align:right;">
1.26
</td>
<td style="text-align:right;">
-0.51
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.48
</td>
<td style="text-align:right;">
-0.17
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.16
</td>
<td style="text-align:right;">
1.30
</td>
<td style="text-align:right;">
-0.45
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
0.51
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.41
</td>
<td style="text-align:right;">
1.23
</td>
<td style="text-align:right;">
-0.50
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.60
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
-0.33
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.87
</td>
<td style="text-align:right;">
0.73
</td>
<td style="text-align:right;">
0.53
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.72
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.48
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.30
</td>
<td style="text-align:right;">
0.90
</td>
<td style="text-align:right;">
-0.11
</td>
<td style="text-align:right;">
-0.44
</td>
<td style="text-align:right;">
-0.63
</td>
<td style="text-align:right;">
-0.11
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.97
</td>
<td style="text-align:right;">
0.68
</td>
<td style="text-align:right;">
0.61
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
-0.44
</td>
<td style="text-align:right;">
-0.29
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
-1.31
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.20
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.84
</td>
<td style="text-align:right;">
1.42
</td>
<td style="text-align:right;">
1.31
</td>
<td style="text-align:right;">
0.96
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
-0.46
</td>
<td style="text-align:right;">
0.17
</td>
</tr>
<tr>
<td style="text-align:right;">
-2.75
</td>
<td style="text-align:right;">
1.89
</td>
<td style="text-align:right;">
0.45
</td>
<td style="text-align:right;">
-0.30
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
0.23
</td>
<td style="text-align:right;">
-0.20
</td>
</tr>
<tr>
<td style="text-align:right;">
0.87
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
0.38
</td>
<td style="text-align:right;">
-0.93
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
0.20
</td>
<td style="text-align:right;">
-0.03
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.44
</td>
<td style="text-align:right;">
2.32
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
-0.19
</td>
<td style="text-align:right;">
-0.74
</td>
<td style="text-align:right;">
-0.19
</td>
<td style="text-align:right;">
-0.09
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.87
</td>
<td style="text-align:right;">
0.71
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
-0.80
</td>
<td style="text-align:right;">
-0.14
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
2.97
</td>
<td style="text-align:right;">
0.26
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
-0.08
</td>
</tr>
<tr>
<td style="text-align:right;">
2.77
</td>
<td style="text-align:right;">
0.70
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
0.60
</td>
<td style="text-align:right;">
0.02
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
2.23
</td>
<td style="text-align:right;">
0.77
</td>
<td style="text-align:right;">
-0.06
</td>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
0.10
</td>
</tr>
<tr>
<td style="text-align:right;">
1.64
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:right;">
-0.89
</td>
<td style="text-align:right;">
-0.25
</td>
<td style="text-align:right;">
0.56
</td>
<td style="text-align:right;">
-0.02
</td>
<td style="text-align:right;">
-0.08
</td>
</tr>
<tr>
<td style="text-align:right;">
1.12
</td>
<td style="text-align:right;">
1.05
</td>
<td style="text-align:right;">
-0.38
</td>
<td style="text-align:right;">
-0.01
</td>
<td style="text-align:right;">
-0.82
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
0.21
</td>
</tr>
<tr>
<td style="text-align:right;">
3.97
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
0.30
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
-0.31
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
-0.24
</td>
</tr>
<tr>
<td style="text-align:right;">
3.21
</td>
<td style="text-align:right;">
0.67
</td>
<td style="text-align:right;">
-0.17
</td>
<td style="text-align:right;">
-0.03
</td>
<td style="text-align:right;">
0.06
</td>
<td style="text-align:right;">
-0.25
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
1.69
</td>
<td style="text-align:right;">
0.86
</td>
<td style="text-align:right;">
-0.42
</td>
<td style="text-align:right;">
-0.10
</td>
<td style="text-align:right;">
0.17
</td>
<td style="text-align:right;">
0.05
</td>
<td style="text-align:right;">
-0.05
</td>
</tr>
<tr>
<td style="text-align:right;">
2.21
</td>
<td style="text-align:right;">
0.85
</td>
<td style="text-align:right;">
-0.35
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
-0.56
</td>
<td style="text-align:right;">
0.22
</td>
<td style="text-align:right;">
0.01
</td>
</tr>
<tr>
<td style="text-align:right;">
2.03
</td>
<td style="text-align:right;">
1.03
</td>
<td style="text-align:right;">
0.21
</td>
<td style="text-align:right;">
0.09
</td>
<td style="text-align:right;">
-0.32
</td>
<td style="text-align:right;">
-0.17
</td>
<td style="text-align:right;">
-0.02
</td>
</tr>
<tr>
<td style="text-align:right;">
3.39
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
-0.16
</td>
<td style="text-align:right;">
0.31
</td>
<td style="text-align:right;">
-0.34
</td>
<td style="text-align:right;">
0.04
</td>
<td style="text-align:right;">
0.26
</td>
</tr>
<tr>
<td style="text-align:right;">
2.71
</td>
<td style="text-align:right;">
0.61
</td>
<td style="text-align:right;">
0.03
</td>
<td style="text-align:right;">
0.47
</td>
<td style="text-align:right;">
-0.15
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
0.07
</td>
</tr>
</tbody>
</table>
</div>
<p><code>sdev</code> denotes to the standard deviations of the principal components -</p>
<pre class="r"><code>pc_prcomp$sdev^2   # eigenvalues (variances)</code></pre>
<pre><code># [1] 4.50218058 1.77420306 0.28306214 0.19330296 0.14637109 0.06596874 0.03491141</code></pre>
<pre class="r"><code>apply(pc_prcomp$x, 2, var)  # variance of the principal components</code></pre>
<pre><code>#        PC1        PC2        PC3        PC4        PC5        PC6        PC7 
# 4.50218058 1.77420306 0.28306214 0.19330296 0.14637109 0.06596874 0.03491141</code></pre>
<p>In <code>rotation</code>, the columns are actually eigen vectors -</p>
<pre class="r"><code>pc_prcomp$rotation</code></pre>
<pre><code>#           PC1         PC2         PC3         PC4         PC5         PC6
# x1 -0.4015216  0.22330581 -0.77779224  0.11376703 -0.09709303  0.39818828
# x2 -0.4228354 -0.26263576 -0.21829956 -0.25341590 -0.26839234 -0.72641133
# x3 -0.4382952 -0.13669090  0.15742089 -0.61921884  0.22529984  0.24584891
# x4 -0.4229505 -0.27070943  0.35156231  0.07051460  0.17588063  0.33970948
# x5 -0.1100766  0.70615050  0.02748501 -0.15839631  0.57425067 -0.29742388
# x6 -0.4385403 -0.08385339  0.13877297  0.71318889  0.25070443 -0.21049616
# x7 -0.2908030  0.53245569  0.42307656  0.02655884 -0.66650427  0.07182518
#            PC7
# x1 -0.05504745
# x2 -0.20161965
# x3  0.51942077
# x4 -0.68769497
# x5 -0.21252061
# x6  0.40693163
# x7  0.05327880</code></pre>
<div id="individual-points-plot" class="section level3">
<h3>Individual Points Plot</h3>
<p>If we visualize the data by different colors for different level of family we will be able to identify them in clusters because each family type is likely to contain specific characteristics which are now almost entirely explained by the first two principal components -</p>
<pre class="r fold-hide"><code>library(tidyverse)
data.frame(pc_prcomp$x, Family = data$Family) %&gt;% 
  ggplot(aes(x=PC1, y=PC2, color=Family)) +
  geom_point(size=2) + theme_bw() +
  labs(title = &quot;First 2 prinipal components&quot;,
       x = &quot;Principal Component 1&quot;, y =&quot;Principal Component 2&quot;,
       col = &quot;Family&quot;) </code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-26-1.png" width="576" /></p>
</div>
<div id="d-plot" class="section level3">
<h3>3D Plot</h3>
<p>The following 3D plot shows the points for the first 3 principal components -</p>
<pre class="r fold-hide"><code>library(plotly)
df2 &lt;- data.frame(pc_prcomp$x, Family = data$Family)
df2 %&gt;% 
  plot_ly(x = ~PC1, y = ~PC2, z = ~PC3, color = ~Family) %&gt;% 
  add_markers(
         hovertemplate = paste(
           &#39;&lt;br&gt;&lt;b&gt;PC1&lt;/b&gt;: %{x:.2f}&#39;,
           &#39;&lt;br&gt;&lt;b&gt;PC2&lt;/b&gt;: %{y:.2f}&#39;,
           &#39;&lt;br&gt;&lt;b&gt;PC3&lt;/b&gt;: %{z:.2f}&lt;/b&gt;&#39;)) %&gt;% 
  layout(
    # setting axis titles
    scene = list(xaxis = list(title = &#39;PC1&#39;),
                 yaxis = list(title = &#39;PC2&#39;),
                 zaxis = list(title = &#39;PC3&#39;)),
    legend = list(title=list(text=&#39;&lt;b&gt; Family &lt;/b&gt;&#39;),
                  orientation = &quot;h&quot;,   # show entries horizontally
                  xanchor = &quot;center&quot;,  # use center of legend as anchor
                  x = 0.5))            # put legend in center of x-axis </code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"37a4102b608d":["function () ","plotlyVisDat"]},"cur_data":"37a4102b608d","attrs":{"37a4102b608d":{"x":{},"y":{},"z":{},"color":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","hovertemplate":"<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"PC1"},"yaxis":{"title":"PC2"},"zaxis":{"title":"PC3"}},"legend":{"title":{"text":"<b> Family <\/b>"},"orientation":"h","xanchor":"center","x":0.5},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-2.28027029756031,-2.16152275318037,-1.41256207803447,-0.871774348946678,-1.30486115426938,-0.968111587615747,-0.089451278250562,-1.83774966321666,-2.74637857837561,0.86821198759582,-3.4362050926022,-1.86941896708972],"y":[1.26480633778655,1.30042746622567,1.2288573617005,0.729971271704599,0.897176245858039,0.684381107040771,0.296837984810679,1.41543150547012,1.8857101470007,0.695491329786685,2.32171169617679,0.706980089194195],"z":[-0.509661042802249,-0.447001739684592,-0.50143375858852,0.529044799646244,-0.110843255196401,0.609621278818712,0.665477441932711,1.30721743288707,0.453200858164217,0.377648155775305,0.0853581780566606,0.0887912450996594],"type":"scatter3d","mode":"markers","hovertemplate":["<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>"],"name":"Carangidae","marker":{"color":"rgba(102,194,165,1)","line":{"color":"rgba(102,194,165,1)"}},"textfont":{"color":"rgba(102,194,165,1)"},"error_y":{"color":"rgba(102,194,165,1)"},"error_x":{"color":"rgba(102,194,165,1)"},"line":{"color":"rgba(102,194,165,1)"},"frame":null},{"x":[-1.65690101356603,-0.563632920607686,-2.66680180568779,0.217873302791822,1.80341375801724,0.52874735955463,-2.48964506315707,-1.24217044141841,-0.361087051731567,-1.31601298455762,-1.20798528974387,-2.88342117832094],"y":[-1.60820567866307,-1.87528491316947,-1.30536665484325,-2.56519986618984,-2.70844256467312,-1.85120191584628,-0.639723265465204,-1.93832576005365,-1.49433633565225,-1.81845806915042,-1.38480094056126,-0.993299657870237],"z":[0.453021466620316,0.552544857440501,-0.0508954137330154,0.968030258101784,0.558029344460063,-0.512486540434705,-0.0868104201388693,0.180434854007737,-0.421102655676714,-1.18329951278488,-0.26214561985883,-0.901884723167533],"type":"scatter3d","mode":"markers","hovertemplate":["<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>"],"name":"Serranidae","marker":{"color":"rgba(252,141,98,1)","line":{"color":"rgba(252,141,98,1)"}},"textfont":{"color":"rgba(252,141,98,1)"},"error_y":{"color":"rgba(252,141,98,1)"},"error_x":{"color":"rgba(252,141,98,1)"},"line":{"color":"rgba(252,141,98,1)"},"frame":null},{"x":[2.96827788609163,2.77310216171014,2.23119577780069,1.64459450190699,1.12338149249728,3.96963981581232,3.20991934413603,1.69105804390619,2.20540252475835,2.029026883844,3.38911318960709,2.71300551790248],"y":[0.256585747830747,0.704346171912982,0.765742317668015,0.268906001875263,1.0486718885955,-0.1250157945561,0.668201485110942,0.855167608963715,0.850597512502824,1.02983091021704,-0.182360708201245,0.614189937463066],"z":[0.218137907653434,-0.175660610912939,-0.061230501542883,-0.885393680147606,-0.380939214394128,0.297675420477153,-0.171641432570859,-0.416231773957881,-0.346428081667302,0.209124145600731,-0.161616079485035,0.0333484120026408],"type":"scatter3d","mode":"markers","hovertemplate":["<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>","<br><b>PC1<\/b>: %{x:.2f} <br><b>PC2<\/b>: %{y:.2f} <br><b>PC3<\/b>: %{z:.2f}<\/b>"],"name":"Sparidae","marker":{"color":"rgba(141,160,203,1)","line":{"color":"rgba(141,160,203,1)"}},"textfont":{"color":"rgba(141,160,203,1)"},"error_y":{"color":"rgba(141,160,203,1)"},"error_x":{"color":"rgba(141,160,203,1)"},"line":{"color":"rgba(141,160,203,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="scree-plot" class="section level2">
<h2>Scree plot</h2>
<p>Scree plot is useful in determining the number of principal components to retain in a principal component analysis or the number of factors to keep in an exploratory factor analysis-<sup><a href="https://en.wikipedia.org/wiki/Scree_plot">4</a></sup></p>
<p>Using the manual calculation’s output <code>eigcorr$values</code> to draw a scree plot -</p>
<pre class="r"><code>plot(x = 1:7, y = eigcorr$values, type = &quot;b&quot;, pch = 19,
     col=&quot;#952C22&quot;, main = &quot;Scree plot&quot;, las = 1, 
     ylab = &quot;Variances Explained&quot;, xlab = &quot;&quot;,
     xaxt = &quot;n&quot;  # to hide x-axis labels
     )
axis(side = 1, at = 1:7, labels = paste0(&quot;PC&quot;,1:7))</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>In 1966, <a href="https://doi.org/10.1207%2Fs15327906mbr0102_10">Raymond B. Cattell</a> invented the scree plot. Scree plot is used to identify statistically significant components. The process is also known as scree test.</p>
<p>According to the scree test, the components that are at the left side of the elbow are to be retained as significant.<a href="https://books.google.com/books?id=FlXwIvSHND8C&amp;pg=PA380">7</a></p>
<p>Another way to select the components to retain is inspecting the variance of the principal components. If the variance is greater than the average of all the variances then the component will be retained, which in the case of standardized data is -</p>
<pre class="r"><code>mean(eigcorr$values)</code></pre>
<pre><code># [1] 1</code></pre>
<p>The average will be 1 irrespective of what was used to determine the eigen values and vectors, i.e., var-covariance matrix and correlation matrix if the data was standardized before calculating.</p>
<p>The scree plot -</p>
<pre class="r"><code>plot(x = 1:7, y = eigcorr$values, type = &quot;b&quot;, pch = 19,
     col=&quot;#952C22&quot;, main = &quot;Scree plot&quot;, las = 1, 
     ylab = &quot;Variances Explained&quot;, xlab = &quot;&quot;,
     xaxt = &quot;n&quot;  # to hide x-axis labels
     )
axis(side = 1, at = 1:7, labels = paste0(&quot;PC&quot;,1:7))
abline(h=1, col=&quot;#2E7BF9&quot;)</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>So the first two principal components will be kept.</p>
<p><strong>Extra:</strong></p>
<p>To make a pretty scree plot using ggplot2 with secondary y-axis that shows the percentage of variance explained -</p>
<pre class="r"><code>library(tidyverse)
# creating a tidy data frame
data.frame(PC= paste0(&quot;PC&quot;,1:length(pc_prcomp$sdev)),
           Variance=pc_prcomp$sdev^2) %&gt;%
  # using the data frame to make plot
  ggplot(aes(x=PC, y=Variance, group=1)) +
  geom_point(size=1, col = &quot;red&quot;) +
  geom_line(col = &quot;red&quot;) + theme_bw() +
  scale_y_continuous(
    name = &quot;Variance&quot;,
    # adding a secondary axis
    sec.axis = sec_axis(
      trans = ~ . / sum((pc_prcomp$sdev) ^ 2),
      # name of secondary axis
      name = &quot;Percentage of Variance Explained&quot;,
      labels = function(x) {
        paste0(round(x * 100,), &quot;%&quot;)
      }
    )
    ) +
  geom_line(y = 1, color = &quot;#2E7BF9&quot;, linetype=&quot;dashed&quot;) +
  labs(title=&quot;Scree plot&quot;, x = &quot;Principal Components&quot;)</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Constructing a scree plot using the function <code>fviz_eig()</code> from the <code>factoextra</code> package<sup><a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/#:~:text=The%20function%20princomp()%20uses,preferred%20compared%20to%20princomp().">5</a></sup> -</p>
<pre class="r"><code>library(factoextra)
fviz_screeplot(pc_prcomp, addlabels = T, linecolor = &quot;red&quot;,
               choice = &quot;eigenvalue&quot;, geom = &quot;line&quot;) +
  geom_line(y = 1, color = &quot;#2E7BF9&quot;, linetype=&quot;dashed&quot;) +
  xlab(&quot;Principal Components&quot;)</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<hr />
</div>
<div id="factominers-pca" class="section level2">
<h2>FactoMineR’s PCA()</h2>
<p>Aside from using <code>prcomp()</code>, the function <code>PCA()</code> from FactoMineR package can be used which extracts principal components from the correlation matrix -</p>
<pre class="r"><code>library(FactoMineR)
pc_fmr &lt;- PCA(df_unstd, graph = F, scale.unit = T)</code></pre>
<p>To see the eigenvalues of the components and their corresponding information -</p>
<pre class="r"><code>pc_fmr$eig</code></pre>
<pre><code>#        eigenvalue percentage of variance cumulative percentage of variance
# comp 1 4.50218058             64.3168655                          64.31687
# comp 2 1.77420306             25.3457580                          89.66262
# comp 3 0.28306214              4.0437449                          93.70637
# comp 4 0.19330296              2.7614709                          96.46784
# comp 5 0.14637109              2.0910156                          98.55885
# comp 6 0.06596874              0.9424106                          99.50127
# comp 7 0.03491141              0.4987345                         100.00000</code></pre>
<p>Plotting the individual points of the first two principal components in a scatterplot -</p>
<pre class="r"><code>plot.PCA(pc_fmr, axes = c(1,2))</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
<div id="biplot" class="section level2">
<h2>Biplot</h2>
<p>We have already constructed PCs using different functions in R. The signs of the components’ are not same in all the cases.
If we construct the biplots for the two functions used to calculate PCs (Base R’s prcomp() and FactorMineR’s PCA()) using the function <code>fviz_pca_biplot()</code> from the <code>factoextra</code> we will get -</p>
<pre class="r"><code>ggpubr::ggarrange(
fviz_pca_biplot(pc_fmr,              # using - FactoMineR&#39;s PCA()
                repel = TRUE,        # to avoid overplotting
                axes = c(1, 2),      # To plot 1st and 2nd PC
                col.var = &quot;#2E9FDF&quot;, # Variables color
                col.ind = &quot;#696969&quot;,  # Individuals color
                title = &quot;PCA - Using FactoMineR&#39;s PCA()&quot;
                ),
fviz_pca_biplot(pc_prcomp,           # using - base R&#39;s prcomp()
                repel = TRUE,        # to avoid overplotting
                axes = c(1, 2),      # To plot 1st and 2nd PC
                col.var = &quot;#2E9FDF&quot;, # Variables color
                col.ind = &quot;#696969&quot;,  # Individuals color
                title = &quot;PCA - Using base R&#39;s prcomp()&quot;),
ncol = 1, nrow = 2)</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p><em>Here, if a positive or negative PC is found depending on the function used it just means that the eigenvector is projected pointing in one direction or 180 degree away in the other direction. The interpretation remains same regardless of the sign, because the variance that is explained is unchanged.</em><a href="https://stats.stackexchange.com/questions/88880/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers">6</a></p>
<p>Another visualization using the <code>ggbiplot()</code> function from <code>ggbiplot</code> package -</p>
<pre class="r"><code># remotes::install_github(&quot;vqv/ggbiplot&quot;)
library(ggbiplot)
ggbiplot(pc_fmr, groups=as.factor(data$Family), ellipse = TRUE) +
  labs(title = &quot;Biplot: Using the first two PCs&quot;,
       col = &quot;Family&quot;) + theme_bw()</code></pre>
<p><img src="/2022/03/21/pca-introduction-r/index.en_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>… To be continued</p>
<hr />
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p><a href="http://dni.dali.dartmouth.edu/2tzzoxnk26p8/01-mrs-katlynn-jacobs-iv-2/read-8173814775-multivariate-analysis-and-its-applications.pdf">Textbook</a> Bhuyan KC. Multivariate analysis and its applications. New Central Book Agency; 2005.</p>
<p><a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202">1</a> Jolliffe IT, Cadima J. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 2016 Apr 13;374(2065):20150202.</p>
<p><a href="https://www.datacamp.com/community/tutorials/pca-analysis-r">2</a> Datacamp - Principal Component Analysis in R Tutorial</p>
<p><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579">3</a> StackExchange - Making sense of principal component analysis, eigenvectors &amp; eigenvalues</p>
<p><a href="https://en.wikipedia.org/wiki/Scree_plot">4</a> Scree plot</p>
<p><a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/#:~:text=The%20function%20princomp()%20uses,preferred%20compared%20to%20princomp().">5</a> sthda - Principal Component Methods in R: Practical Guide</p>
<p><a href="https://stats.stackexchange.com/questions/88880/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers">6</a> Does the sign of scores or of loadings in PCA or FA have a meaning? May I reverse the sign?</p>
<p><a href="https://books.google.com/books?id=FlXwIvSHND8C&amp;pg=PA380">7</a>Alex Dmitrienko; Christy Chuang-Stein; Ralph B. D’Agostino (2007). Pharmaceutical Statistics Using SAS: A Practical Guide. SAS Institute. p. 380. ISBN 978-1-59994-357-2.</p>
</div>

    </div>

    
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/r/">R</a>
          <a href="/tags/pca/">PCA</a>
          <a href="/tags/scree-plot/">scree plot</a>
          <a href="/tags/biplot/">biplot</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/2022/03/31/shapiro-wilk-test/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Shapiro Wilk Test using R</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/2021/11/20/statistics-subject-review/">
            <span class="next-text nav-default">পরিসংখ্যান সাবজেক্ট রিভিউ</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'ahsanul';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:ahsanulislam10@email.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/13323413/md-ahsanul-himel" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://twitter.com/mdahsanulhimel" class="iconfont icon-twitter" title="twitter"></a>
      <a href="https://www.facebook.com/MdAhsanulHimel/" class="iconfont icon-facebook" title="facebook"></a>
      <a href="https://www.linkedin.com/in/MdAhsanulHimel/" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="https://github.com/MdAhsanulHimel" class="iconfont icon-github" title="github"></a>
  
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2022 - 
    2023<span class="heart"><i class="iconfont icon-heart"></i></span><span>Md. Ahsanul Islam</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" id="MathJax-script" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
</script>






  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.64437849d125a2d603b3e71d6de5225d641a32d17168a58106e0b61852079683.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-XHMN5CZBNV', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







</body>
</html>
