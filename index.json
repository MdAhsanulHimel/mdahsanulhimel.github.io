
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Md Ahsanul Islam is a passionate statistician with a knack for uncovering insights from data and transforming them into actionable solutions. With expertise in R, SAS, STATA, IBM SPSS, and MS Excel, he excels in data collection, analysis, and interpretation.\nHe has worked with professionals from various fields as well as at a market research agency as a data analyst. Currently he is working with his thesis research as well as preparing himself for higher studies.\nConnect with him on LinkedIn, explore his projects on Github, and discover his data visualizations on Tableau Public for collaboration and insights.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1710246404,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Md Ahsanul Islam is a passionate statistician with a knack for uncovering insights from data and transforming them into actionable solutions. With expertise in R, SAS, STATA, IBM SPSS, and MS Excel, he excels in data collection, analysis, and interpretation.","tags":null,"title":"Md Ahsanul Islam","type":"authors"},{"authors":[],"categories":["Hypothesis Tests"],"content":"Statistical inference এর অনেক গুরুত্বপূর্ণ একটি টপিক হচ্ছে hypothesis testing। এটি পড়তে গিয়ে অনেকের মাঝে একটা প্রশ্ন ঘুরপাক খায় যে level of significance কেন 0.05 ধরতে হবে। p-value এর মান 0.05 এর নিচে হলে কেনো null hypothesis রিজেক্ট করতে হবে? এসবের উত্তরই উঠে আসবে এই লেখায়।\nহাইপোথিসিস টেস্টে দুটো হাইপোথিসিস থাকে। Alternative hypothesis (H1) বলে যে বাস্তবে কোনো পরিবর্তন ঘটেছে বা প্রভাব আছে, আর Null hypothesis (H₀) বলে যে, কোনো পরিবর্তন ঘটেনি বা প্রভাব নেই। এই হাইপোথিসিস টেস্ট করার সময় আরো একটি বিষয় নির্ধারণ করে নিতে হয়, যাকে level of significance (alpha) বলা হয়। সাধারণত এর মান 0.05 ধরা হয়। ক্ষেত্রবিশেষে একে 0.10 কিংবা 0.01 ও ধরা হয়ে থাকে। level of significance আসলে হাইপোথিসিস টেস্টে ডিসিশন নেওয়ার জন্য একটি cutoff point / threshold হিসেবে ব্যবহার হয় । যদি টেস্ট করার পর টেস্ট স্ট্যাটিস্টিক (যেমন t statistic, F-statistic, etc.) এর সাথে সংশ্লিষ্ট p-value এর মান 0.05 এর কম হয় তাহলে আমরা Null hypothesis-কে reject করে দেই। নইলে রিজেক্ট করি না।\nকিন্তু কেন ৫%? ২০১৩ সালে অক্সফোর্ড উনিভার্সিটি প্রেস থেকে রয়াল স্ট্যাটিস্টিক্যাল সোস্যাইটি এর প্রকাশিত ম্যাগাজিন “Significance”-এ এটি নিয়ে একটি আর্টিকেল প্রকাশিত হয়। সেখানে বলা হয় পরিসংখ্যানবিদরা মিলে একমত হয়ে এই 0.05 কে নির্দিষ্ট করেছেন সবার কাজের সুবিধার্থে। কিন্তু তাতেও এর সঠিক উত্তর আসে না। তারা কেনই বা 0.05 কে নির্ধারণ করলো?\nএটিকে বুঝার জন্য আমাদেরকে আরেকটু পেছনে যেতে হবে। সম্ভাবনা থেকে শুরু করা যাক। ধরুন আপনার হাতে ২০ টি কাগজ আছে। এর মাঝে ১০ টি কাগজের এক পাশে লাল কলম দিয়ে আপনার নাম লিখা আছে, বাকি ১০ টিতে কিছু লিখা নেই। যদি আপনি লটারির মাধ্যমে এই ২০ টি কাগজ থেকে একটি কাগজ বাছাই করেন তাহলে আপনি আপনার নাম লিখা কার্ড বাছাই করার সম্ভাবনা ১০/২০ = ০.৫।\nএটিকে যদি একটি জুয়া খেলায় পরিণত করা যায় তাহলে কেমন হবে? আপনার নামযুক্ত কাগজ যদি লটারিতে উঠে তাহলে আপনি জিতবেন, নইলে হারবেন। জিতলে আপনি ১০০ টাকা পাবেন, হারলে ১০০ টাকা দিবেন।\nযদি খেলাটা fair হয়, অর্থাৎ, ১০টি কাগজে নাম আছে আর ১০টি কাগজে নাম নেই, সেক্ষেত্রে লম্বা সময় ধরে খেললে law of large numbers অনুযায়ী আপনার আসলে লাভ-লস কোনোটাই হওয়ার কথা না।\nকিন্তু, যদি জুয়ারী যোচ্চুরি করে ১০ টি নামযুক্ত কাগজ সড়িয়ে ফেলে সবগুলো নামহীন কাগজ ঢুকিয়ে দেয় তাহলে কিন্তু সেখানে কোনো কাগজেই আর আপনার নাম থাকবেনা। অর্থাৎ আপনি প্রত্যেকটা লটারির ড্রতেই হারবেন।\nযদি খেলাটা ফেয়ার হতো অর্থাৎ আপনার হার-জিতের সম্ভাবনা সমান থাকতো, সেক্ষেত্রে ধরে নিচ্ছি আপনার হারার সম্ভাবনা 0.5 (Null hypothesis). কিন্তু যদি খেলাটা ফেয়ার না হয়, সেক্ষেত্রে আপনার জেতার সম্ভাবনা আর 0.5 থাকবে না, হয়তো বেরে যাবে কিংবা কমে যাবে (Alternative hypothesis)।\nএবার জুয়ারির টেবিল থেকে আপনি একটি কাগজ ড্র করলেন আর হেরে গেলেন (কারণ খেলাটা এখন ফেয়ার না, জুয়ারি সেখানে আপনার নামযুক্ত কোনো কাগজই রাখেনি)। কিন্তু আপনি সন্দেহ করলেন না, কারণ আপনি ধরে নিয়েছেন খেলাটা ফেয়ার হচ্ছে এবং আপনার হারার সম্ভাবনা অর্ধেক অর্থাৎ, $0.5$ । দ্বিতীয়বারেও আপনি হারলেন। তৃতীয়বারেও আপনি হারলেন। কিন্তু একটা ফেয়ার গেইমে টানা ৩ বার হারার সম্ভাবনা-\n$0.5 * 0.5 * 0.5 = 0.125$\nআপনি চতুর্থবারেও হারলেন। ফেয়ার গেইমে টানা ৪ বার হারার সম্ভাবনা -\n$0.5 * 0.5 * 0.5 * 0.5 = 0.0625$\nআপনার মাঝে এখন কিছুটা সন্দেহ হচ্ছে যে আসলেই কি গেইমটা ফেয়ার হচ্ছে কিনা। তবে সন্দেহটা এখনো জোরালো না ধরে নিচ্ছি। পঞ্চমবারে এসেও যখন আপনি হারবেন তখন হিসাব করে দেখুন গেইমটা যদি আসলেই ফেয়ার হতো তাহলে টানা ৫ বার হারার সম্ভাবনা কতো?\n$0.5 * 0.5 * 0.5 * 0.5 * 0.5 = 0.03125$\nটানা এতবার হারার পরে আসলে আপনার সন্দেহ প্রবল হয়েছে যে আসলে খেলাটা ফেয়ার হচ্ছে না। অর্থাৎ আপনি null hypothesis রিজেক্ট করে দিয়েছেন যখন দেখলেন যে খেলাটা ফেয়ার হলে টানা এতবার হারার সম্ভাবনা ৪% এরও কম। অর্থাৎ এমন একটা সিচুয়েশনে গিয়ে আপনি নাল হাইপোথেসিস রিজেক্ট করে দিলেন যখন দেখলেন খেলায় টানা হারার সম্ভাবনা (occurrence of an event) এক্সট্রিম লেভেলে (খুবই কম) চলে গেছে given that জুয়া খেলাটা ফেয়ার (null hypothesis)। p-value এর ডেফিনেশন এখন আপনি সহজে বুঝতে পারবেন - “P-value represents the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis is true.”\nবিখ্যাত পরিসংখ্যানবিদ রোনাল্ড ফিশার “Statistical Methods for Research Workers”-এ এই সিগনিফিক্যান্স লেভেলের ধারণাটি পোষণ করেন। পরবর্তীতে বিভিন্ন সময়ে টেস্ট স্ট্যাটিস্টিকের ভিত্তিতে ক্রিটিক্যাল ভেলু ক্যালকুলেট করা হয় ১%, ৫% এবং ১০% level of significance-এ। এভাবেই নানান এক্সপেরিমেন্ট এবং থিওরিটিক্যাল ভ্যালিডেশনের মাধ্যমে পরবর্তীতে সময়ের সাথে ৫% কে স্ট্যান্ডার্ড হিসেবে ধরে নেওয়া হয়।\n","date":1728518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728563308,"objectID":"a409aecefecf751e01676710bb70d69b","permalink":"http://example.org/post/2024-10-10-why-5-significance-level/","publishdate":"2024-10-10T00:00:00Z","relpermalink":"/post/2024-10-10-why-5-significance-level/","section":"post","summary":"হাইপোথেসিস টেস্টিংয়ে, ০.০৫ সিগনিফিক্যান্স লেভেলকে একটি মানদণ্ড হিসেবে ধরা হয়, যেখানে p-value এই মানের নিচে হলে নাল হাইপোথেসিস (H₀) রিজেক্ট করা হয়। রোনাল্ড ফিশার এই কনসেপ্টটি প্রতিষ্ঠিত করেন, যা পরীক্ষামূলক ও থিওরিটিক্যাল গবেষণার মাধ্যমে জনপ্রিয় হয়। এই পোস্টে একটি প্রেক্টিক্যাল এপ্লিকেশনের মাধ্যমে এর intuition বুঝানো হয়েছে।","tags":["hypothesis test","level of significance"],"title":"Why 5% Significance Level?","type":"post"},{"authors":null,"categories":["Multivariate Analysis"],"content":" Now, \\(trace(\\Sigma)\\)\n\\(=trace(\\Gamma D\\Gamma\u0026#39;)\\)\n\\(=trace(\\Gamma\u0026#39;\\Gamma D)\\) [Cyclic property] \\(=trace(D)\\) [Since, \\(\\Gamma\\) is an orthogonal matrix]\n\\(=\\sum_{i=1}^{p}\\lambda_i\\)\nCluster Analysis (CA) Cluster analysis is a data analysis technique that aims to identify and group observations or data points that are homogeneous, meaning they share similar characteristics or properties. It is a data mining technique and is considered an unsupervised learning algorithm that helps reveal underlying patterns and structures within the data.\nCluster analysis is a powerful tool that can be used for a variety of tasks, including:\nCustomer segmentation: Cluster analysis can be used to segment customers into groups based on their purchase behavior, demographics, or other factors. This can help businesses to target their marketing campaigns more effectively. Fraud detection: Cluster analysis can be used to identify fraudulent transactions by finding groups of transactions that are similar in terms of their characteristics. Product recommendation: Cluster analysis can be used to recommend products to users based on their past purchase history or other factors. Anomaly detection: Cluster analysis can be used to identify anomalies or outliers in data. This can be helpful for identifying fraud, detecting system errors, or identifying new trends. Types Centroid-based clustering and Hierarchical clustering are two distinct methods commonly used in clustering analysis.\nIn centroid-based clustering, the number of clusters is predetermined and specified before the clustering process begins. The algorithm aims to partition the observations into the specified number of clusters by minimizing the within-cluster sum of squares.\nIn contrast, hierarchical clustering does not require specifying the number of clusters in advance. It generates a tree-like structure called a dendrogram, which shows the hierarchical relationships between the observations at different levels of similarity. By cutting the dendrogram at different heights, we can obtain clusterings for different numbers of clusters, ranging from 1 to the total number of observations.\nCentroid-based Clustering This is a bottom-up approach to clustering, where objects are assigned to non-overlapping clusters based on their proximity to a central point, called a centroid. The most commonly centroid-based clustering algorithms include k-means, k-medoids, and fuzzy c-means.\nHierarchical Clustering Hierarchical clustering is a top-down approach to clustering that creates a hierarchy of clusters by recursively merging or splitting them based on their similarity. It can be either agglomerative (where objects are merged together to form clusters) or divisive (where clusters are split apart to form smaller clusters).\nR Code Step 1: Load Data df \u0026lt;- read.csv(\u0026#34;df_coffee.csv\u0026#34;) Taking a sample of 20 observations for the demonstration purpose:\nset.seed(100) df \u0026lt;- df[sample(1:nrow(df), size = 20),] Always check descriptive statistics:\nskimr::skim(df) Table: Table 1: Data summary\nName df Number of rows 20 Number of columns 16 _______________________ Column type frequency: character 1 numeric 15 ________________________ Group variables None Variable type: character\nskim_variable n_missing complete_rate min max empty n_unique whitespace Color 0 1 5 12 0 7 0 Variable type: numeric\nskim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Aroma 0 1 7.75 0.35 7.17 7.48 7.75 8.00 8.50 ▇▆▇▆▃ Flavor 0 1 7.77 0.34 7.17 7.56 7.75 7.94 8.50 ▃▃▇▃▁ Aftertaste 0 1 7.59 0.31 7.00 7.40 7.58 7.77 8.17 ▂▆▇▅▂ Acidity 0 1 7.67 0.30 7.08 7.50 7.71 7.81 8.25 ▂▃▇▃▁ Body 0 1 7.66 0.25 7.17 7.48 7.67 7.83 8.17 ▂▃▇▃▂ Balance 0 1 7.67 0.33 7.17 7.42 7.67 7.85 8.25 ▅▃▇▃▃ Uniformity 0 1 10.00 0.00 10.00 10.00 10.00 10.00 10.00 ▁▁▇▁▁ Clean.Cup 0 1 10.00 0.00 10.00 10.00 10.00 10.00 10.00 ▁▁▇▁▁ Sweetness 0 1 10.00 0.00 10.00 10.00 10.00 10.00 10.00 ▁▁▇▁▁ Overall 0 1 7.67 0.38 7.17 7.33 7.62 7.92 8.50 ▇▃▃▁▃ Defects 0 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ▁▁▇▁▁ Total.Cup.Points 0 1 83.77 2.09 80.17 82.50 83.70 84.89 87.58 ▅▇▇▆▅ Moisture.Percentage 0 1 10.94 1.31 9.00 9.70 11.20 11.83 13.10 ▇▃▃▇▃ Category.One.Defects 0 1 0.15 0.49 0.00 0.00 0.00 0.00 2.00 ▇▁▁▁▁ Quakers 0 1 0.35 1.14 0.00 0.00 0.00 0.00 5.00 ▇▁▁▁▁ Three of the variables have no variability (standard deviation of zero). So they should be removed:\ndf \u0026lt;- dplyr::select(df, -c(\u0026#34;Uniformity\u0026#34;,\u0026#34;Clean.Cup\u0026#34;,\u0026#34;Sweetness\u0026#34;,\u0026#34;Defects\u0026#34;)) We can only use numeric variables in cluster analysis functions in R for now. (There are some advanced techniques that discuss methods related to cluster analysis in mixed data types).\ndf_num \u0026lt;- df[,-ncol(df)] # removing the last variable in the dataset which is character type Step 2: Complete Cases Take only complete cases:\ndf_com \u0026lt;- na.omit(df_num) Step 3: Scaling Scaling the dataset is important because different features may have different scales, and if they are not scaled, then the features with larger scales will have a …","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688389782,"objectID":"0f2ad219793f9c9931fac6b8fa994e05","permalink":"http://example.org/post/cluster-analysis/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/post/cluster-analysis/","section":"post","summary":"Discover the power of cluster analysis in R with this comprehensive blog post. Learn the essential techniques, interpret the results, and gain valuable insights from your data.","tags":["R","Cluster Analysis"],"title":"Cluster Analysis","type":"post"},{"authors":null,"categories":["Design of Experiment","Multivariate Analysis"],"content":" Intro MANOVA (Multivariate analysis of variance) is an extension of ANOVA (Analysis of Variance) technique, which is used to compare means across different groups for a single dependent variable. In MANOVA, the dependent variables are correlated, and the analysis takes into account their relationships to provide a more comprehensive understanding of the effects of the independent variables. If you have only a single dependent variable, then MANOVA is not required. Regular ANOVA would be more appropriate in such cases.\nAlthough separate F tests could be computed for each outcome variable, multiple hypothesis testing would result in an increased likelihood of at least one Type I error. In addition, because multiple outcome variables are likely to be correlated, the structure underlying the system of outcomes would be ignored with separate tests and statistical power could be decreased. A more appropriate way to examine two or more outcome variables would be to take a multivariate approach.$^6$\nExample For example, if you want to determine if the height of individuals varies across different age groups, ANOVA would be suitable since you have a single dependent variable (height).\nHowever, if you have multiple dependent variables that are related to the same set of independent variables, then using MANOVA would be advantageous. For instance, suppose you have data on the height, weight, and body fat percentage of individuals, and you want to investigate if these variables differ based on different exercise routines. In this scenario, MANOVA would be appropriate as you have multiple dependent variables (height, weight, and body fat percentage) that are of interest and are potentially influenced by the independent variable (exercise routine).\nAssumptions The dependent variables are assumed to follow a multivariate normal distribution. This assumption is important for the validity of the statistical tests used in MANOVA. The observations should be independent of each other. There should be no multivariate outliers in the dependent variables. The dependent variables are linearly related. Hypothesis \\(H_0:\\) μ1 = ··· = μk against \\(H_1:\\) not all μi’s are the same.$^3$\nor,\n\\(H_0:\\) Group mean vectors are all equal to one another.\n\\(H_1:\\) At least one pair of treatments is different on at least one variable.\nMANOVA Test Statistics Wilk’s Lambda: \\(\\Lambda^* = \\dfrac{|\\mathbf{E}|}{|\\mathbf{H+E}|}\\) Hotelling-Lawley Trace: \\(T^2_0 = trace(\\mathbf{HE}^{-1})\\) Pillai Trace: \\(V = trace(\\mathbf{H(H+E)^{-1}})\\) Roy’s Maximum Root: Largest eigenvalue of \\(HE^{-1}\\) The power functions of these tests depend on the eigen values. Mardia (1971) has shown that Pillai’s test is robust to nonnormality so long as the distribution is symmetrc, John (1967) has shown that under some conditions, Pillai’s test is also locally most powerful. \\(^7\\)\nExperiment Data Description The data is from an experiment in which the optimal conditions for growth and product formation were determined for a bacterial strain in a broth with a certain carbon source. Two different nitrogen sources (yeast extract and ammonium chloride) and three different incubation temperatures (30, 35, 37) were evaluated. Bacterial growth was evaluated after 24 hours using dry cell weight (in mg/ml) and optical density at 600 nm. The yield of a desired fermentation product was determined using gas chromatography and expressed in mM. \\(^4\\)\n# reading data data \u0026lt;- read.table(\u0026#34;data/MANOVA.txt\u0026#34;, header = T, sep = \u0026#34;\\t\u0026#34;) library(tidyverse) data \u0026lt;- data %\u0026gt;% mutate(across(c(\u0026#34;Temperature\u0026#34;,\u0026#34;N.source\u0026#34;), factor)) summary(data) # Experiment Temperature N.source Replica # Length:120 30:40 ammonium chloride:60 Min. : 1.00 # Class :character 35:40 yeast extract :60 1st Qu.: 5.75 # Mode :character 37:40 Median :10.50 # Mean :10.50 # 3rd Qu.:15.25 # Max. :20.00 # Dry.weight Optical.density Product.yield # Min. : 1.670 Min. :0.210 Min. : 8.40 # 1st Qu.: 4.805 1st Qu.:1.770 1st Qu.:41.90 # Median : 6.070 Median :1.910 Median :57.65 # Mean : 6.024 Mean :1.976 Mean :57.10 # 3rd Qu.: 7.080 3rd Qu.:2.223 3rd Qu.:72.70 # Max. :10.300 Max. :2.480 Max. :93.10 data %\u0026gt;% group_by(Temperature) %\u0026gt;% summarise(Mean_dry_weight = mean(Dry.weight), Mean_optical_density = mean(Optical.density), Mean_product_yield = mean(Product.yield)) # # A tibble: 3 × 4 # Temperature Mean_dry_weight Mean_optical_density Mean_product_yield # \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; # 1 30 5.92 1.92 56.6 # 2 35 5.93 1.99 56.5 # 3 37 6.22 2.01 58.3 Hypothesis \\(H_0:\\) There is no significant difference in the average levels of dry weight, optical density, and product yield among two treatment groups.\nMANOVA manova_result \u0026lt;- manova(cbind(Dry.weight, Optical.density, Product.yield) ~ Temperature, data = data) summary(manova_result, test = \u0026#34;Pillai\u0026#34;) # Df Pillai approx F num Df den Df Pr(\u0026gt;F) # Temperature 2 0.030774 0.60427 6 232 0.7268 # Residuals 117 summary(manova_result, test = \u0026#34;Wilks\u0026#34;) # Df Wilks approx F num Df den Df Pr(\u0026gt;F) # Temperature 2 …","date":1685318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709569329,"objectID":"ee9b483a1d9251136e201a1275e988be","permalink":"http://example.org/post/manova/","publishdate":"2023-05-29T00:00:00Z","relpermalink":"/post/manova/","section":"post","summary":"MANOVA (Multivariate analysis of variance) is an extension of ANOVA (Analysis of Variance) technique, which is used to compare means across different groups for a single dependent variable. In MANOVA, the dependent variables are correlated, and the analysis takes into account their...","tags":["R","MANOVA"],"title":"MANOVA Analysis: Unveiling Multivariate Patterns with R and Python","type":"post"},{"authors":["Club Executive"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1675256400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"http://example.org/talk/inauguration-of-programming-club/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/inauguration-of-programming-club/","section":"event","summary":"Inauguration of Programming Club at the Dept. of Statistics, University of Chittagong","tags":[],"title":"Inauguration of Programming Club","type":"event"},{"authors":null,"categories":["Tech"],"content":" To use R in Jupyter Notebook, make sure you have the following things installed in your computer -\nAnaconda distribution R Now, to use R in jupyter notebook, follow the steps -\nInstall IRkernal package.\ninstall.packages(\u0026#39;IRkernel\u0026#39;)\nInstall kernal spec for the current user.\nIRkernel::installspec()\nIf you pass user = FALSE inside the function, then the spec will be installed for all users/system wide. Done! 😄 👏\n","date":1658188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"1855ecd337d8af38de3ef5b37ba3129c","permalink":"http://example.org/post/r-in-jupyter-notebook/","publishdate":"2022-07-19T00:00:00Z","relpermalink":"/post/r-in-jupyter-notebook/","section":"post","summary":"In this post, I have showed complete procedure of enabling R kernal for Jupyter Notebook, so that you can use R in Jupyter Notebook.","tags":["R","JupyterNotebook"],"title":"Use R in Jupyter Notebook","type":"post"},{"authors":null,"categories":["Biostatistics"],"content":" The Kaplan–Meier estimator, often known as the product limit estimator, is a non-parametric statistic used to estimate the survival function using lifetime data. In this post, we will see how to estimate the survival function in this method using R.\nPackages \u0026amp; Data For fitting a Kaplan-Meier curve, the survival package is required -\nlibrary(survival) For this example, I will use the addicts dataset -\nremotes::install_github(\u0026#34;lbraglia/suanselete3\u0026#34;) data(addicts, package = \u0026#39;suanselete3\u0026#39;) Here, the package suanselete3 is an unofficial companion to the textbook “Survival Analysis - A Self-Learning Text” by D.G. Kleinbaum and M. Klein (3rd Ed., 2012) including all the accompanying datasets. I have loaded the addicts data set from this package.\nDescription of the data (event=dropped out) -\nID – Patient ID CLINIC – Indicates which methadone treatment clinic the patient attended (coded 1 or 2) STATUS – Indicates whether the patient dropped out of the clinic (coded 1) or was censored (coded 0) SURVT – The time (in days) until the patient dropped out of the clinic or was censored PRISON – Indicates whether the patient had a prison record (coded 1) or not (coded 0) DOSE – A continuous variable for the patient’s maximum methadone dose (mg/day) Data Preparation Let’s have an idea about the data set -\nhead(addicts) # id clinic status survt prison dose # 1 1 1 1 428 0 50 # 2 2 1 1 275 1 55 # 3 3 1 1 262 0 55 # 4 4 1 1 183 0 30 # 5 5 1 1 259 1 65 # 6 6 1 1 714 0 55 Check the class of the data set imported in R -\nclass(addicts) # [1] \u0026#34;data.frame\u0026#34; It is a data frame. To work with this, firstly, we need to create a Surv object which will be used as a response variable -\nY \u0026lt;- Surv(time = addicts$survt, event = addicts$status == 1) Y # [1] 428 275 262 183 259 714 438 796+ 892 393 161+ 836 # [13] 523 612 212 399 771 514 512 624 209 341 299 826+ # [25] 262 566+ 368 302 602+ 652 293 564+ 394 755 591 787+ # [37] 739 550 837 612 581+ 523 504 785 774 560 160 482 # [49] 518 683 147 563 646 899 857 180 452 760 496 258 # [61] 181 386 439+ 563+ 337 613+ 192 405+ 667 905+ 247 821 # [73] 821 517+ 346+ 294 244 95 376 212 96 532 522 679 # [85] 408+ 840+ 148+ 168 489 541+ 205 475+ 237 517 749 150 # [97] 465 708 713+ 146+ 450 555+ 460 53+ 122 35 532+ 684+ # [109] 769+ 591+ 769+ 609+ 932+ 932+ 587+ 26 72+ 641+ 367+ 633+ # [121] 661 232 13 563+ 969+ 1052+ 944+ 881+ 190 79 884+ 170 # [133] 286 358+ 326+ 769+ 161 564+ 268 611+ 322 1076+ 2+ 788+ # [145] 575+ 109 730+ 790+ 456+ 231 143 86+ 1021+ 684+ 878 216 # [157] 808+ 268 222+ 683+ 496+ 389 126 17 350 531+ 317+ 461+ # [169] 37 167 358 49 457 127 7 29 62 150+ 223 129+ # [181] 204+ 129 581 176 30 41 543+ 210+ 193 434 367 348 # [193] 28+ 337+ 175+ 149 546 84 283+ 533 207 216 28+ 67 # [205] 62+ 111+ 257 136 342+ 41 531+ 98+ 145 50 53+ 103+ # [217] 2+ 157 75 19 35 394+ 117 175 180 314 480+ 325+ # [229] 280 204 366 531+ 59 33 540 551+ 90 47 Here the time variable survt has been assigned with the plus sign (+) indicating censored and not censored otherwise (based on status variable).\nModel In the following code, Y~1 requests an intercept only model -\nkmfit1 = survfit(Y~1) kmfit1 # Call: survfit(formula = Y ~ 1) # # n events median 0.95LCL 0.95UCL # 238 150 504 399 560 The output contains descriptive information on the number of records, the number at risk at time 0, the number of events, and the median estimated survival time with a 95% confidence interval.\nThe summary function shows the survival estimates -\nsummary(kmfit1) # Call: survfit(formula = Y ~ 1) # # time n.risk n.event survival std.err lower 95% CI upper 95% CI # 7 236 1 0.996 0.00423 0.9875 1.000 # 13 235 1 0.992 0.00597 0.9799 1.000 # 17 234 1 0.987 0.00729 0.9731 1.000 # 19 233 1 0.983 0.00840 0.9667 1.000 # 26 232 1 0.979 0.00937 0.9606 0.997 # 29 229 1 0.975 0.01026 0.9546 0.995 # 30 228 1 0.970 0.01107 0.9488 0.992 # 33 227 1 0.966 0.01182 0.9431 0.989 # 35 226 2 0.957 0.01317 0.9320 0.984 # 37 224 1 0.953 0.01379 0.9265 0.981 # 41 223 2 0.945 0.01493 0.9158 0.974 # 47 221 1 0.940 0.01546 0.9105 0.971 # 49 220 1 0.936 0.01597 0.9053 0.968 # 50 219 1 0.932 0.01646 0.9001 0.965 # 59 216 1 0.927 0.01694 0.8949 0.961 # 62 215 1 0.923 0.01740 0.8897 0.958 # 67 213 1 0.919 0.01785 0.8845 0.954 # 75 211 1 0.914 0.01829 0.8793 0.951 # 79 210 1 0.910 0.01871 0.8742 0.948 # 84 209 1 0.906 0.01913 0.8691 0.944 # 90 207 1 0.901 0.01953 0.8639 0.940 # 95 206 1 0.897 0.01992 0.8588 0.937 # 96 205 1 0.893 0.02029 0.8537 0.933 # 109 202 1 0.888 0.02067 0.8486 0.930 # 117 200 1 0.884 0.02104 0.8435 0.926 # 122 199 1 0.879 0.02140 0.8384 0.922 # 126 198 1 0.875 0.02174 0.8333 0.919 # 127 197 1 0.870 0.02208 0.8282 0.915 # 129 196 1 0.866 0.02241 0.8232 0.911 # 136 194 1 0.862 0.02274 0.8181 0.907 # 143 193 1 0.857 0.02305 0.8131 0.903 # 145 192 1 0.853 0.02336 0.8080 0.900 # 147 190 1 0.848 0.02366 0.8030 0.896 # 149 188 1 0.844 0.02396 0.7979 0.892 # 150 187 1 0.839 0.02426 0.7929 0.888 # 157 185 1 0.835 0.02455 0.7878 0.884 # 160 184 1 0.830 0.02483 …","date":1655769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"13e92fdf0f41e783116e2406477ecce5","permalink":"http://example.org/post/kaplan-meier-estimatior-in-r/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/post/kaplan-meier-estimatior-in-r/","section":"post","summary":"The Kaplan–Meier estimator, often known as the product limit estimator, is a non-parametric statistic used to estimate the survival function using lifetime data. In this post, we will see how to estimate the survival function in this method using R.","tags":["R","Biostatisics"],"title":"The Kaplan-Meier Estimatior in R","type":"post"},{"authors":null,"categories":["Resources"],"content":" Nothing is better resource than a book. So, in this post, I’ve shared some books that have been extremely helpful to me in my quest to learn multivariate analysis.\nMethods of Multivariate Statistics by M.S. Srivastava.\nRead the book online or download the pdf.\n{width=250px} An Introduction to Multivariate Statistical Analysis by T. W. Anderson\nDownload pdf\n{width=250px} Applied Multivariate Statistical Analysis by Richard A. Johnson and Dean W. Wichern Download pdf\n{width=250px} ","date":1655596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"e0849f8e043e805c1ba805df79ec6502","permalink":"http://example.org/post/books-multivariate-analysis/","publishdate":"2022-06-19T00:00:00Z","relpermalink":"/post/books-multivariate-analysis/","section":"post","summary":"Nothing is better resource than a book. So, in this post, I've shared some books that have been extremely helpful to me in my quest ...","tags":["Statistics","Multivariate Analysis"],"title":"Recommended Books For Multivariate Analysis","type":"post"},{"authors":null,"categories":["Hypothesis Tests"],"content":" The normality of data is crucial in parametric hypothesis tests because all the parametric tests are based on normal distribution. The central limit theorem states that if the sample size is sufficiently large, the data will be normally distributed. But it may not be attained in all the cases. Typically, analysis needs to be performed on a small number of samples. As a result, checking for normality is a necessary step.\nSamuel Sanford Shapiro and Martin Wilk developed a test in 1965 for checking the normality of data, which was later named after them as Shapiro-Wilk test. It is appropriate when sample size is less than 50. For data greater than that Shapiro and Francia in 1972 developed a new testing procedure. For now we will stick to Shapiro-Wilk test.\nShapiro-Wilk Test using R Suppose we have the data on age of 18 randomly selected students in a primary school. We need to test whether the data follows a normal distribution or not.\nData -\nage = c(9,12,11,10,12,9,7,8,8,7,10,10,8,7,9,10,11,12) age # [1] 9 12 11 10 12 9 7 8 8 7 10 10 8 7 9 10 11 12 Graphically representing the data in a histogram -\nhist(age) From the visualization, it is not clear whether the data follows a normal distribution or not because the sample size is small.\nHypothesis formulation - $H_0$: The data follows a normal distribution with unknown mean and variance.\n$H_1$: The data does not follow a normal distribution. We want to test the hypothesis at 5% significance level.\nHypothesis testing -\nshapiro.test(x = age) # # Shapiro-Wilk normality test # # data: age # W = 0.92157, p-value = 0.1378 Since the p-value is 0.1378 \u0026gt; significance level (0.05), we may not reject the null hypothesis. It means we do not have enough statistical evidence to reject the null hypothesis that the data come from a normal distribution at 5% level of significance.\n","date":1648684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"e9c73c6786c5657a6b498ff0f10193a0","permalink":"http://example.org/post/shapiro-wilk-test/","publishdate":"2022-03-31T00:00:00Z","relpermalink":"/post/shapiro-wilk-test/","section":"post","summary":"The normality of data is crucial in parametric hypothesis tests. Because all the parametric tests are based on normal distribution. The central limit theorem states that if the sample size is sufficiently large, the data will be normally distributed. But it may not be attained in all the cases. Typically, an analysis needs to be performed...","tags":["shapiro wilk test","normality test","hypothesis test","R"],"title":"Shapiro Wilk Test using R","type":"post"},{"authors":null,"categories":["Others"],"content":" প্রথমেই যেটা জানার বিষয় সেটা হচ্ছে এটা Science এর সাবজেক্ট। স্কুল কলেজে কমার্সের সাবজেক্ট বলে পরিচিত হলেও এটা বিজ্ঞান বিভাগের বিষয়। অর্থাৎ এখান থেকে আপনি পাবেন Bachelor of Science Honours সার্টিফিকেট।\nপাবলিক বিশ্ববিদ্যালয়ে যাদের সিরিয়াল মেরিটলিস্টে একটু নিচের দিকে, তাদের অনেকেই বিভিন্ন পিওর সাবজেক্ট নিয়ে থাকে। Statistics সে সকল পিওর subjects এর একটি। ব্যক্তিগতভাবে আমি মনে করি পিওর সাবজেক্ট হিসেবে অন্যান্য যেকোন সাবজেক্টের তুলনায় পরিসংখ্যানের এপ্লিকেশন, ডিমান্ড অনেক বেশী।\nবেশীরভাগ স্টুডেন্টেরই এই সাবজেক্ট নিয়ে খুব একটা ধারণা নেই। সত্যি বলতে ভর্তির আগে আমি নিজেও খুব বেশী কিছু জানতাম না এই সাবজেক্ট নিয়ে। এমনকি ব্যাবসায় শিক্ষার স্টুডেন্টদের এটা পড়তে দেখে ভাবতাম এটা ওদের সাবজেক্ট, সায়েন্সের না।\nজানা যাক কি কি পড়ানো হয় এই সাবজেক্টে? Basic Statistics, Probability, Estimation, Statistical Inference, Sampling Techniques, Research Methodology, Econometrics, Demography, Stochastic Process, Actuarial Statistics, Biostatistics \u0026amp; Epidemology, Data Mining, Industrial Statistics, Multivariate Analysis, Operational Research ইত্যাদি থাকে মেজর সাবজেক্ট হিসেবে।\nএগুলোর সাথে নন-মেজর হিসেবে ম্যাথের বিভিন্ন কোর্স যেমন: Algebra, Calculus(এটা অনেক অনেক অনেক প্রেক্টিসে রাখতে হয়), Linear Algebra(পদে পদে এটা থাকবে), Numerical Mathematics ইত্যাদি। আরো রয়েছে Economics, Language(English)। এই নন মেজরগুলোতে আয়ত্ব রাখতে পারলে মেজরে ভালো করা অনেক সহজ হয়ে যায়। যা ওভারঅলভাবে সাবজেক্টের উপর দক্ষতা বৃদ্ধি করে।\nট্যাকনিক্যাল নলেজের জন্য ফার্স্ট ইয়ার থেকেই বিভিন্ন Statistical Software এর কোর্স করানো হয়। যদিও সেগুলো শুধু বেসিক ধারণাটাই দিবে। নিজেকেই প্রেক্টিস করে করে আরো ভালো ধারণা নিয়ে সেগুলোতে এক্সপার্ট হতে হবে। এগুলোর মাঝে রয়েছে MS Excel, STATA, R, SPSS, SAS ইত্যাদি। সবমিলিয়ে মোট ১৬০ ক্রেডিট এর চার বছরের B.Sc. কোর্স সিলেবাস তৈরী করা হয়েছে।\nদেশের বাইরে সুযোগ কেমন?\nবর্তমান সময়টা ডাটা সায়েন্স এর সময়। USA, EU, Canada এর ইউনিভার্সিটিগুলোতে স্নাতকোত্তর ও তদুর্ধ্ব উচ্চশিক্ষার জন্য রেজাল্ট এর ভিত্তিতে স্কলারশিপ পাওয়া যায়। দেশের বাইরে ওয়ার্কপ্লেসে বেতন ও ডিমান্ডের কথা চিন্তা করলে Statistics সেরাদের মাঝেই রয়েছে। দেশেও বিভিন্ন রিসার্চ ওরিয়েন্টেড কাজ স্ট্যাটিস্টিশিয়ান ছাড়া অচল।\nকি চাকরি করবো?\nবিভিন্ন গবেষণা প্রতিষ্ঠান, ব্যাংক, আর্থিক প্রতিষ্ঠানে Statistics এর গ্রাজুয়েটদের রয়েছে প্রচুর চাহিদা। Private sector এর Bank, Insurance, গবেষণা ও পরিকল্পনা প্রতিষ্ঠানগুলো Statistics ছাড়া একেবারেই অচল। Bangladesh Bank এর AD (general) এর পাশাপাশি AD (Statistics) এবং AD (Research) নামে আলাদা দুটি ফিল্ড আছে যাতে শুধুমাত্র Statistics ও Economics এর গ্রাজুয়েটরাই Apply করতে পারে। CDDR, B, NIPORT সহ অনেক বড় বড় রিসার্চ প্রতিষ্ঠানে Statistics এর স্টুডেন্টদের খুবই ডিমান্ড। বিভিন্ন আন্তর্জাতিক প্রতিষ্ঠান, মাল্টিন্যাশনাল কোম্পানিতে রিসার্চের কাজে পরিসংখ্যানবিদ অত্যাবশ্যক। BCS Professional Cadre এ শিক্ষকতা আছে। যদিও বিগত বছরগুলোতে সিট কম ছিলো। এছাড়া প্রতিটি থানায় একজন পরিসংখ্যান অফিসারের পদ আছে। শূণ্য থাকার ভিত্তিতে নিয়োগ পাওয়া সম্ভব। IT থেকে শুরু করে economics, automobile, pharma company/­medicine, psychology, marketing, public health, biology এমনকি প্রায় প্রতিটা ইন্ড্রাস্ট্রিতে product development, manufacturing, quality control etc sector এ চাহিদার কথা নাই বলাই বাহুল্য। হয়তো সপ্ন ছিলো CSE পড়বে, Google, Apple, Microsoft ইত্যাদি জায়ান্ট কোম্পানিতে জব করবে। চিন্তা নেই! ভালো করে সাবজেক্টটা আয়ত্ত্ব করো। এসব কোম্পানিতে রয়েছে Statistician, Quantitative Analyst, Market Analyst, Data Scientists ইত্যাদি পদ যেখানে শুধুমাত্র CSE \u0026amp; Statistics গ্রাজুয়েটরাই apply করতে পারে।\nপরিসংখ্যান বিষয়টি আসলে এমন এক বিষয় যে বিষয়ের গ্রাজুয়েটদের ছাড়া প্রায় সব ধরনের ব্যবসা, শিল্প ও গবেষণা প্রতিষ্ঠান অচল। মৌলিক বিষয়গুলোর মধ্যে এটি এমন এক বিষয় যার বাণিজ্যিক চাহিদা সব সময়ই অন্যগুলো থেকে বেশি। তাই এই বিষয়ে ক্যারিয়ার গঠনে রয়েছে ভালো সম্ভাবনা।\nআর সবশেষে একটাই কথা। বিশ্ববিদ্যালয়ে কোন পড়াশোনা নেই এটা মিথ্যা কথা। বিশ্ববিদ্যালয়ে পড়াশোনা কলেজের চেয়েও বেশী। যেই বিষয়েই পড় না কেন, পড়ার চাপ ১৯-২০ কমবেশী হতে পারে। বিষয়টাকে ভালোবাসলে সবই সহজ, সবই ভালো।\nMd. Ahsanul Islam Department of Statistics University of Chittagong\n","date":1637366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"0695a1b220358123ad30e300f5fc84bc","permalink":"http://example.org/post/statistics-subject-review/","publishdate":"2021-11-20T00:00:00Z","relpermalink":"/post/statistics-subject-review/","section":"post","summary":"পরিসংখ্যান বিষয়টি আসলে এমন এক বিষয় যে বিষয়ের গ্রাজুয়েটদের ছাড়া প্রায় সব ধরনের ব্যবসা, শিল্প ও গবেষণা প্রতিষ্ঠান অচল। মৌলিক বিষয়গুলোর মধ্যে এটি এমন এক বিষয় যার বাণিজ্যিক চাহিদা সব সময়ই অন্যগুলো থেকে বেশি। তাই এই বিষয়ে ক্যারিয়ার গঠনে...","tags":["Statistics","Subject review"],"title":"পরিসংখ্যান সাবজেক্ট রিভিউ","type":"post"},{"authors":null,"categories":["R Programming"],"content":" Importing the ggplot2 package -\nlibrary(ggplot2) Let’s use the built in Hair and Eye Color data set -\nHairEyeColor , , Sex = Male Eye Hair Brown Blue Hazel Green Black 32 11 10 3 Brown 53 50 25 15 Red 10 10 7 7 Blond 3 30 5 8 , , Sex = Female Eye Hair Brown Blue Hazel Green Black 36 9 5 2 Brown 66 34 29 14 Red 16 7 7 7 Blond 4 64 5 8 This data set is not so suitable for visualization. So we need to do some manipulation before moving on.\nLet’s import some necessary packages -\nlibrary(dplyr) The data set is then transformed into a form so that we can use it for plotting -\ndf \u0026lt;- HairEyeColor %\u0026gt;% as_tibble() %\u0026gt;% tidyr::uncount(n) %\u0026gt;% mutate_all(as.factor) More about uncount -\ntibble(a=c(2,1,4), b=c(\u0026#39;one\u0026#39;,\u0026#39;two\u0026#39;,\u0026#39;three\u0026#39;)) %\u0026gt;% tidyr::uncount(a) # A tibble: 7 x 1 b \u0026lt;chr\u0026gt; 1 one 2 one 3 two 4 three 5 three 6 three 7 three Uncount does the opposite work of count.\nLet’s see the new data frame now-\nglimpse(df) Rows: 592 Columns: 3 $ Hair \u0026lt;fct\u0026gt; Black, Black, Black, Black, Black, Black, Black, Black, Black, Bl~ $ Eye \u0026lt;fct\u0026gt; Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown, Brown, Br~ $ Sex \u0026lt;fct\u0026gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Male,~ Now it can be used to create bar charts.\nA Simple Barplot ggplot(data = df) + geom_bar(mapping = aes(x = Hair)) The mapping can be done inside the ggplot() function -\nggplot(data = df, mapping = aes(x=Hair))+ geom_bar(fill = \u0026#34;black\u0026#34;) + labs(title = \u0026#34;Hair Color\u0026#34;, subtitle = \u0026#34;592 Statistics Students\u0026#34;, caption = \u0026#34;(From R\u0026#39;s built in HairEyeColor sample dataset)\u0026#34;, y = \u0026#34;Number of Students\u0026#34;, x = NULL) Horizontal Bar Chart Using coord_flip() -\nggplot(data = df, mapping = aes(x=Hair))+ geom_bar(fill = \u0026#34;black\u0026#34;) + labs(title = \u0026#34;Hair Color\u0026#34;, subtitle = \u0026#34;592 Statistics Students\u0026#34;, caption = \u0026#34;(From R\u0026#39;s built in HairEyeColor sample dataset)\u0026#34;, y = \u0026#34;Number of Students\u0026#34;, x = NULL) + coord_flip() Assigning variable to the y axis -\nggplot(data = df, mapping = aes(y = Hair))+ geom_bar(fill = \u0026#34;black\u0026#34;) + labs(title = \u0026#34;Hair Color\u0026#34;, subtitle = \u0026#34;592 Statistics Students\u0026#34;, caption = \u0026#34;(From R\u0026#39;s built in HairEyeColor sample dataset)\u0026#34;, y = \u0026#34;Number of Students\u0026#34;, x = NULL) Using Colors fill = {the same variable as the x axis} so that for each variable different colors is shown -\nggplot(data = df)+ geom_bar(mapping = aes(x = Hair, fill = Hair))+ theme(legend.position = \u0026#34;none\u0026#34;) # Don\u0026#39;t show the legend Using hue -\nggplot(data = df)+ geom_bar(mapping = aes(x = Hair, fill = Hair))+ theme(legend.position = \u0026#34;none\u0026#34;) + # Don\u0026#39;t show the legend scale_fill_hue(c = 20) # Different values c gives different intensity of colors Manually selecting colors How to manually set colors in a bar chart?\nManually selecting colors -\nggplot(data = df)+ geom_bar(mapping = aes(x = Hair, fill = Hair), col = \u0026#34;black\u0026#34;, fill = c(\u0026#34;Black\u0026#34;,\u0026#34;beige\u0026#34;,\u0026#34;bisque3\u0026#34;,\u0026#34;red\u0026#34;))+ theme(legend.position = \u0026#34;none\u0026#34;) Another way to do that -\nggplot(data = df)+ geom_bar(mapping = aes(x = Hair, fill = Hair), col = \u0026#34;black\u0026#34;)+ theme(legend.position = \u0026#34;none\u0026#34;) + scale_fill_manual(values = c(\u0026#34;Black\u0026#34;,\u0026#34;beige\u0026#34;,\u0026#34;bisque3\u0026#34;,\u0026#34;red\u0026#34;)) Modifying Axis Tickmarks ggplot(df, aes(x = Hair)) + geom_bar() + scale_y_continuous(breaks = seq(0, 300, by=50)) + labs(x = \u0026#34;Colors\u0026#34;, y = \u0026#34;Frequency\u0026#34;, title = \u0026#34;Bar Chart of Colors\u0026#34;, subtitle = \u0026#34;An observational study\u0026#34;) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) # center the title and subtitle Stacked Bar Chart Using fill argument stacked bar can be made -\nggplot(data = df) + geom_bar(mapping = aes(Hair, fill = Sex)) 100% Stacked Bar Chart Using position = “fill” inside geom_bar -\nggplot(df, aes(Hair, fill = Sex)) + geom_bar(position = \u0026#34;fill\u0026#34;) + labs(x=\u0026#34;Hair Color\u0026#34;, y=NULL) + coord_flip() Changing Order of Bars df$Hair \u0026lt;- factor(df$Hair, levels = c(\u0026#34;Red\u0026#34;, \u0026#34;Black\u0026#34;, \u0026#34;Blond\u0026#34;, \u0026#34;Brown\u0026#34;)) ggplot(df, aes(y=Hair, fill = Sex)) + geom_bar(position = \u0026#34;fill\u0026#34;) + labs(x=NULL, y=\u0026#34;Hair Color\u0026#34;) Another way to do this using scale_y_discrete()-\nggplot(df, aes(y = Hair, fill = Sex)) + geom_bar(position = \u0026#34;fill\u0026#34;) + labs(x=NULL, y=\u0026#34;Hair Color\u0026#34;) + scale_y_discrete(limits = c(\u0026#34;Black\u0026#34;,\u0026#34;Red\u0026#34;,\u0026#34;Brown\u0026#34;,\u0026#34;Blond\u0026#34;)) Changing Order in Legend’s Labels Using scale_fill_discrete() -\nggplot(df, aes(y = Hair, fill = Sex)) + geom_bar(position = \u0026#34;fill\u0026#34;) + labs(x=NULL, y=\u0026#34;Hair Color\u0026#34;) + scale_y_discrete(limits = c(\u0026#34;Black\u0026#34;,\u0026#34;Red\u0026#34;,\u0026#34;Brown\u0026#34;,\u0026#34;Blond\u0026#34;)) + scale_fill_discrete(breaks = c(\u0026#34;Male\u0026#34;,\u0026#34;Female\u0026#34;)) Changing Order of Stacks In the following stacked barplot, the left bar denotes female and the right bar denotes male -\nggplot(df, aes(x = Hair, fill = Sex)) + geom_bar(position = \u0026#34;dodge\u0026#34;) + labs(x=NULL, y=\u0026#34;Hair Color\u0026#34;) + scale_x_discrete(limits = c(\u0026#34;Black\u0026#34;,\u0026#34;Red\u0026#34;,\u0026#34;Brown\u0026#34;,\u0026#34;Blond\u0026#34;)) If we check the order of levels of Sex we’ll see -\nlevels(df$Sex) [1] \u0026#34;Female\u0026#34; \u0026#34;Male\u0026#34; Now if the order is changed, the bar will also change its order -\ndf %\u0026gt;% mutate(Sex = factor(Sex, levels = c(\u0026#34;Male\u0026#34;,\u0026#34;Female\u0026#34;))) %\u0026gt;% ggplot(aes(x = Hair, fill = Sex)) + geom_bar(position = \u0026#34;dodge\u0026#34;) + labs(x=NULL, y=\u0026#34;Hair Color\u0026#34;) + scale_x_discrete(limits = …","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"b3bad3aa717f8b8f5d0ba2d415d0ef69","permalink":"http://example.org/post/ggplot2-bar-plot/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/post/ggplot2-bar-plot/","section":"post","summary":"R is great for data visualization. One of the amazing packages in visualization in R is the ggplot2. Hadley Wickham created ggplot2 in 2005 as an implementation of Leland Wilkinson's Grammar of Graphics. It divides graphs into semantic components like scales and layers.","tags":["R","ggplot2","bar plot"],"title":"Complete guide to bar plot using ggplot2","type":"post"},{"authors":null,"categories":["R Programming"],"content":" R is great for data visualization. One of the amazing packages in visualization in R is the ggplot2. Hadley Wickham created ggplot2 in 2005 as an implementation of Leland Wilkinson’s Grammar of Graphics. It divides graphs into semantic components like scales and layers.\nAfter reading this post, you’ll be able to create beautiful scatter plots like the one below. Getting Started Install library using install.packages(\u0026#34;ggplot2\u0026#34;).\nIf you’ve already installed it in your computer then load it -\nlibrary(\u0026#34;ggplot2\u0026#34;) Loading the data set and do some changes to make it usable -\ncollege \u0026lt;- read.csv(\u0026#39;Data/college.csv\u0026#39;, stringsAsFactors = TRUE) You can get the data set here\nCalling ggplot() function alone just creates a blank canvas -\nggplot() Adding geom_point layer to the ggplot object to create a scatter plot -\nAdding a layer to the ggplot object with argument geom=\u0026#39;point\u0026#39; -\nggplot(data = college) + layer(geom = \u0026#39;point\u0026#39;, stat = \u0026#34;identity\u0026#34;, position = \u0026#34;identity\u0026#34;, mapping = aes(x = tuition, y = sat_avg)) But the easier and widely used way of adding a layer is using geom_* -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg)) Shape You can change the shape of the points from black dot to something else. For example -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg), shape = 1) You can use different shapes for different values/levels. For example in our college data there is a column named control, that has the information on whether a school is public or private.\nSo if you want to differentiate public vs. private schools but shape you can do that using the shape argument inside the aesthetic mapping -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, shape = control)) Color You can change the color of the points from black dot to something else. For example -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg), color = \u0026#39;darkorchid1\u0026#39;) You can know all the color names by running the code colors()\nSimilar to changing shape based on the levels of a variable, you can also change color. For this you need to pass the color argument inside the aesthetic mapping specifying the variable name -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control)) Now you can clearly see how the private and public schools are performing.\nManually Changing Color You can assign colors of your choice to plot using the function scale_color_manual() -\nmanu_colors \u0026lt;- c(\u0026#34;#FF8C32\u0026#34;, \u0026#34;#06113C\u0026#34;) ggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control))+ scale_color_manual(values = manu_colors) You can hide the legend using the argument show.legend outside of the aesthetic mapping -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control), show.legend = FALSE)+ scale_color_manual(values = manu_colors) colourpicker Addin for choosing color View this link for details on how to install and use this.\nCPCOLS \u0026lt;- c(\u0026#34;#8B0A50\u0026#34;, \u0026#34;#9A32CD\u0026#34;) ggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control))+ scale_color_manual(values=CPCOLS) Point Size You can change the size of the points from regular size to something else. For example -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg), size = 2) Let’s alter the size of pointers in accordance to the number of undergraduates in each point -\nggplot(data = college) + geom_point(aes(x = tuition, y = sat_avg, size = undergrads)) Adding Transparency/Alpha The transparency of the points can be controlled using the argument alpha outside of the aesthetic mapping -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, size = undergrads), alpha = 0.35) alpha takes values from 0 to 1.\nNotice how transparency of the points in the legend also changes. To remove any transparency we can use the guides() function to override the aesthetic of the point -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, size = undergrads), alpha = 0.35) + guides(size = guide_legend(override.aes = list(alpha = 1))) Title \u0026amp; Subtitle Add title and subtitle using the ggtitle -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control, size = undergrads), alpha = 0.35) + ggtitle(\u0026#34;SAT Average score VS Tuition Fee\u0026#34;, subtitle = \u0026#34;A comparison study\u0026#34;) I prefer using labs() because it gives more space to customization, for example changing label of legends -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control, size = undergrads), alpha = 0.35) + labs(title = \u0026#34;SAT Average score VS Tuition Fee\u0026#34;, subtitle = \u0026#34;A comparison study\u0026#34;) Alignment of Title \u0026amp; Subtitle To align the title and subtitle in the middle you can customize the theme manually -\nggplot(data = college) + geom_point(mapping = aes(x = tuition, y = sat_avg, color = control, size = undergrads), alpha = 0.35) + labs(title = …","date":1621728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"487251c52963ab4fc304b6c0acbb8534","permalink":"http://example.org/post/ggplot2-scatter-plot/","publishdate":"2021-05-23T00:00:00Z","relpermalink":"/post/ggplot2-scatter-plot/","section":"post","summary":"R is great for data visualization. One of the amazing packages in visualization in R is the ggplot2. Hadley Wickham created ggplot2 in 2005 as an implementation of Leland Wilkinson's Grammar of Graphics. It divides graphs into semantic components like scales and layers.","tags":["R","ggplot2","tidyverse","scatter plot"],"title":"Complete guide to scatter plot using ggplot2","type":"post"},{"authors":null,"categories":["Stochastic Process"],"content":" Intro to the problem In this post I will show a practical example of markov chain.\nLet’s try to map the movement of freelancer drivers in Dhaka. We can divide the area of Dhaka into three zones – North Dhaka, Middle Dhaka and South Dhaka.\nSo we can find the following probabilities for the movement of a driver:\nAmong the North Dhaka’s freelancer drivers, 30% will remain in North Dhaka, 30% will move to Middle Dhaka, while the remaining 40% will go to South Dhaka. Of all the drivers in Middle Dhaka, 50% and 30% will move to North Dhaka and South Dhaka, respectively; 20% will remain in Middle Zone. In the South Dhaka, drivers have a 40% chance of moving to North Dhaka, 40% chance of staying in the South Dhaka; 20% drivers will move to Middle Dhaka. Once a driver is in a particular zone, he can either move to the next zone or stay back in the same zone. His movement will be decided only by his current state and not by the sequence of past states.\nThe state space in this example includes North Dhaka, Middle Dhaka and South Dhaka. It follows all the properties of Markov Chains because the current state has the power to predict the next stage.\nTransition Probability Matrix Let’s construct the transition probability matrix -\nzone \u0026lt;- c(\u0026#34;North\u0026#34;,\u0026#34;Middle\u0026#34;,\u0026#34;South\u0026#34;) zoneTPM \u0026lt;- matrix(c(0.3,0.3,0.4, 0.5,0.2,0.3, 0.4,0.2,0.4), nrow=3, byrow=T, dimnames = list(zone,zone)) Hence, the TPM is -\nzoneTPM # North Middle South # North 0.3 0.3 0.4 # Middle 0.5 0.2 0.3 # South 0.4 0.2 0.4 The package named markovchain can help us in implementing Markov Chains in R.\nInstall and load the package -\n# install.packages(\u0026#34;markovchain\u0026#34;) library(markovchain) Now using the function new() create a Markov Chain object -\ndriverzone \u0026lt;- new(\u0026#34;markovchain\u0026#34;, states = zone, transitionMatrix = zoneTPM, name = \u0026#34;Driver Movement\u0026#34;) driverzone # Driver Movement # A 3 - dimensional discrete Markov Chain defined by the following states: # North, Middle, South # The transition matrix (by rows) is defined as follows: # North Middle South # North 0.3 0.3 0.4 # Middle 0.5 0.2 0.3 # South 0.4 0.2 0.4 Transition Diagram To plot the above transition matrix we can use R package, diagram. The package has a function called plotmat() that can help us in plotting a state space diagram of the transition matrix in an easy-to-understand manner.\nInstall and load the package -\n# install.packages(\u0026#34;diagram\u0026#34;) library(diagram) Code for plotting the transition probability diagram -\nplotmat(A = zoneTPM, box.type = \u0026#34;circle\u0026#34;, # shape of box box.lwd = 1, # border density of box relsize = 0.9, # scaling factor for size of graph cex.txt = 0.7, # size of probabilities lwd = 1, # border density of state to state arrows lcol = \u0026#34;black\u0026#34;, box.col = \u0026#34;cornsilk1\u0026#34;, box.size = 0.1, # size of box box.prop = 0.6, # height to width ratio of box arr.length = 0.5, arr.width = 0.2, self.cex = 0.5, # size of self probability box self.shifty = -0.03, self.shiftx = 0.13, # location of self prob. box name = c(\u0026#34;North Dhaka\u0026#34;,\u0026#34;Middle Dhaka\u0026#34;,\u0026#34;South Dhaka\u0026#34;), # Optional main = \u0026#34;Transition Diagram\u0026#34;, cex.main = 1.3 # relative size of main title ) The above Markov Chain can be used to answer some of the future state questions.\nQuestions Assuming that a driver is currently in the North Zone, what is the probability that the driver will again be in the North Zone after two trips?\nAnswer:\nManually:\nA driver can reach the North Zone again in his second trip in three different ways:\nStaying in the same zone i.e. North Dhaka to North Dhaka Probability: P(N-N) = 0.3*0.3 = 0.09 Middle Dhaka to North Dhaka Probability: P(M-N) = 0.4*0.4 = 0.12 South Dhaka to North Dhaka Probability: P(S-N) = 0.5*0.4 = 0.2 Therefore, probability of second trip to North = 0.09 + 0.12 + 0.20 = 0.41 (0.40 approximately)\nUsing code:\nUsing the markov chain object created earlier using the function new() -\ndriverzone^2 # Driver Movement^2 # A 3 - dimensional discrete Markov Chain defined by the following states: # North, Middle, South # The transition matrix (by rows) is defined as follows: # North Middle South # North 0.40 0.23 0.37 # Middle 0.37 0.25 0.38 # South 0.38 0.24 0.38 This gives us the direct probability of a driver coming back to the North Zone after two trips.\nThe calculation can be done for subsequent trips as well. For example, the probability of coming back to North Zone in 4th trip :\ndriverzone^3 # Driver Movement^3 # A 3 - dimensional discrete Markov Chain defined by the following states: # North, Middle, South # The transition matrix (by rows) is defined as follows: # North Middle South # North 0.383 0.240 0.377 # Middle 0.388 0.237 0.375 # South 0.386 0.238 0.376 However if we increase n (No. of trips) the predictive power tends to go down, where the Markov Chain reaches an equilibrium called stationary state. In the above case, after 9 trips the state becomes stationary -\ndriverzone^9 # Driver Movement^9 # A 3 - dimensional discrete Markov Chain defined by the following states: # North, Middle, South # The transition matrix …","date":1621728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"4bee88ba64e98caf8282d25301d36074","permalink":"http://example.org/post/markov-chain/","publishdate":"2021-05-23T00:00:00Z","relpermalink":"/post/markov-chain/","section":"post","summary":"Markov Chain is a special type of stochastic process. It is a popular and straightforward way to statistically model a discrete-time, discrete space stochastic processes. In this post I will show a practical example of markov chain. Let’s try to map the movement of freelancer drivers in Dhaka. We can divide the area of Dhaka into three zones – North Dhaka, Middle Dhaka... ","tags":["markov chain","R"],"title":"Markov Chain - A Practical Example","type":"post"},{"authors":["Md Ahsanul Islam"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"http://example.org/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://example.org/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"http://example.org/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"http://example.org/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Md Ahsanul Islam","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"http://example.org/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Md Ahsanul Islam","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687373478,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"http://example.org/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]