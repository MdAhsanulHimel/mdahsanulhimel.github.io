
<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#types" id="toc-types">Types</a>
<ul>
<li><a href="#centroid-based-clustering" id="toc-centroid-based-clustering">Centroid-based Clustering</a></li>
<li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering">Hierarchical Clustering</a></li>
</ul></li>
<li><a href="#r-code" id="toc-r-code">R Code</a>
<ul>
<li><a href="#step-1-load-data" id="toc-step-1-load-data">Step 1: Load Data</a></li>
<li><a href="#step-2-complete-cases" id="toc-step-2-complete-cases">Step 2: Complete Cases</a></li>
<li><a href="#step-3-scaling" id="toc-step-3-scaling">Step 3: Scaling</a></li>
<li><a href="#step-4-optimal-number-of-clusters" id="toc-step-4-optimal-number-of-clusters">Step 4: Optimal Number of Clusters</a>
<ul>
<li><a href="#elbow-method" id="toc-elbow-method">Elbow Method</a></li>
<li><a href="#silhouette-scorewidth" id="toc-silhouette-scorewidth">Silhouette Score/Width</a></li>
<li><a href="#gap-statistic-method" id="toc-gap-statistic-method">Gap Statistic Method</a></li>
<li><a href="#combination-of-all" id="toc-combination-of-all">Combination of All</a></li>
</ul></li>
<li><a href="#step-5-hierarchical-clustering" id="toc-step-5-hierarchical-clustering">Step 5: Hierarchical Clustering</a>
<ul>
<li><a href="#step-5.1-distance" id="toc-step-5.1-distance">Step 5.1: Distance</a></li>
<li><a href="#step-5.2-linkage-criteria" id="toc-step-5.2-linkage-criteria">Step 5.2: Linkage Criteria</a></li>
<li><a href="#step-5.3-dendrogram" id="toc-step-5.3-dendrogram">Step 5.3: Dendrogram</a></li>
</ul></li>
<li><a href="#step-5-k-means-clustering" id="toc-step-5-k-means-clustering">Step 5: K-means Clustering</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<hr />
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Cluster analysis is a data analysis technique that aims to identify and group observations or data points that are homogeneous, meaning they share similar characteristics or properties. It is a data mining technique and is considered an unsupervised learning algorithm that helps reveal underlying patterns and structures within the data.</p>
<p>Cluster analysis is a powerful tool that can be used for a variety of tasks, including:</p>
<ul>
<li>Customer segmentation: Cluster analysis can be used to segment customers into groups based on their purchase behavior, demographics, or other factors. This can help businesses to target their marketing campaigns more effectively.<br />
</li>
<li>Fraud detection: Cluster analysis can be used to identify fraudulent transactions by finding groups of transactions that are similar in terms of their characteristics.<br />
</li>
<li>Product recommendation: Cluster analysis can be used to recommend products to users based on their past purchase history or other factors.<br />
</li>
<li>Anomaly detection: Cluster analysis can be used to identify anomalies or outliers in data. This can be helpful for identifying fraud, detecting system errors, or identifying new trends.</li>
</ul>
<hr />
</div>
<div id="types" class="section level1">
<h1>Types</h1>
<p>Centroid-based clustering and Hierarchical clustering are two distinct methods commonly used in clustering analysis.</p>
<p>In centroid-based clustering, the number of clusters is predetermined and specified before the clustering process begins. The algorithm aims to partition the observations into the specified number of clusters by minimizing the within-cluster sum of squares.</p>
<p>In contrast, hierarchical clustering does not require specifying the number of clusters in advance. It generates a tree-like structure called a dendrogram, which shows the hierarchical relationships between the observations at different levels of similarity. By cutting the dendrogram at different heights, we can obtain clusterings for different numbers of clusters, ranging from 1 to the total number of observations.</p>
<p><img src="index.en_insertimage_1.png" /></p>
<div id="centroid-based-clustering" class="section level2">
<h2>Centroid-based Clustering</h2>
<p>This is a bottom-up approach to clustering, where objects are assigned to non-overlapping clusters based on their proximity to a central point, called a centroid. The most commonly centroid-based clustering algorithms include k-means, k-medoids, and fuzzy c-means.</p>
<p><img src="index.en_insertimage_2.png" /></p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>Hierarchical Clustering</h2>
<p>Hierarchical clustering is a top-down approach to clustering that creates a hierarchy of clusters by recursively merging or splitting them based on their similarity. It can be either agglomerative (where objects are merged together to form clusters) or divisive (where clusters are split apart to form smaller clusters).</p>
<p><img src="index.en_insertimage_3.png" /></p>
<hr />
</div>
</div>
<div id="r-code" class="section level1">
<h1>R Code</h1>
<div id="step-1-load-data" class="section level2">
<h2>Step 1: Load Data</h2>
<pre class="r"><code>df &lt;- read.csv(&quot;df_coffee.csv&quot;)</code></pre>
<p>Taking a sample of 20 observations for the demonstration purpose:</p>
<pre class="r"><code>set.seed(100)
df &lt;- df[sample(1:nrow(df), size = 20),]</code></pre>
<p>Always check descriptive statistics:</p>
<pre class="r"><code>skimr::skim(df)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">df</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">20</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">16</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">15</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<colgroup>
<col width="19%" />
<col width="13%" />
<col width="19%" />
<col width="5%" />
<col width="5%" />
<col width="8%" />
<col width="12%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Color</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">7</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="10%" />
<col width="15%" />
<col width="6%" />
<col width="5%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Aroma</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.75</td>
<td align="right">0.35</td>
<td align="right">7.17</td>
<td align="right">7.48</td>
<td align="right">7.75</td>
<td align="right">8.00</td>
<td align="right">8.50</td>
<td align="left">▇▆▇▆▃</td>
</tr>
<tr class="even">
<td align="left">Flavor</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.77</td>
<td align="right">0.34</td>
<td align="right">7.17</td>
<td align="right">7.56</td>
<td align="right">7.75</td>
<td align="right">7.94</td>
<td align="right">8.50</td>
<td align="left">▃▃▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">Aftertaste</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.59</td>
<td align="right">0.31</td>
<td align="right">7.00</td>
<td align="right">7.40</td>
<td align="right">7.58</td>
<td align="right">7.77</td>
<td align="right">8.17</td>
<td align="left">▂▆▇▅▂</td>
</tr>
<tr class="even">
<td align="left">Acidity</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.67</td>
<td align="right">0.30</td>
<td align="right">7.08</td>
<td align="right">7.50</td>
<td align="right">7.71</td>
<td align="right">7.81</td>
<td align="right">8.25</td>
<td align="left">▂▃▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">Body</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.66</td>
<td align="right">0.25</td>
<td align="right">7.17</td>
<td align="right">7.48</td>
<td align="right">7.67</td>
<td align="right">7.83</td>
<td align="right">8.17</td>
<td align="left">▂▃▇▃▂</td>
</tr>
<tr class="even">
<td align="left">Balance</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.67</td>
<td align="right">0.33</td>
<td align="right">7.17</td>
<td align="right">7.42</td>
<td align="right">7.67</td>
<td align="right">7.85</td>
<td align="right">8.25</td>
<td align="left">▅▃▇▃▃</td>
</tr>
<tr class="odd">
<td align="left">Uniformity</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">0.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="left">▁▁▇▁▁</td>
</tr>
<tr class="even">
<td align="left">Clean.Cup</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">0.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="left">▁▁▇▁▁</td>
</tr>
<tr class="odd">
<td align="left">Sweetness</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">0.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="right">10.00</td>
<td align="left">▁▁▇▁▁</td>
</tr>
<tr class="even">
<td align="left">Overall</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.67</td>
<td align="right">0.38</td>
<td align="right">7.17</td>
<td align="right">7.33</td>
<td align="right">7.62</td>
<td align="right">7.92</td>
<td align="right">8.50</td>
<td align="left">▇▃▃▁▃</td>
</tr>
<tr class="odd">
<td align="left">Defects</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="left">▁▁▇▁▁</td>
</tr>
<tr class="even">
<td align="left">Total.Cup.Points</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">83.77</td>
<td align="right">2.09</td>
<td align="right">80.17</td>
<td align="right">82.50</td>
<td align="right">83.70</td>
<td align="right">84.89</td>
<td align="right">87.58</td>
<td align="left">▅▇▇▆▅</td>
</tr>
<tr class="odd">
<td align="left">Moisture.Percentage</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10.94</td>
<td align="right">1.31</td>
<td align="right">9.00</td>
<td align="right">9.70</td>
<td align="right">11.20</td>
<td align="right">11.83</td>
<td align="right">13.10</td>
<td align="left">▇▃▃▇▃</td>
</tr>
<tr class="even">
<td align="left">Category.One.Defects</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.15</td>
<td align="right">0.49</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">2.00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">Quakers</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.35</td>
<td align="right">1.14</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">5.00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<p>Three of the variables have no variability (standard deviation of zero). So they should be removed:</p>
<pre class="r"><code>df &lt;- dplyr::select(df, -c(&quot;Uniformity&quot;,&quot;Clean.Cup&quot;,&quot;Sweetness&quot;,&quot;Defects&quot;))</code></pre>
<p>We can only use numeric variables in cluster analysis functions in R for now. (There are some advanced techniques that discuss methods related to cluster analysis in mixed data types).</p>
<pre class="r"><code>df_num &lt;- df[,-ncol(df)]  # removing the last variable in the dataset which is character type</code></pre>
</div>
<div id="step-2-complete-cases" class="section level2">
<h2>Step 2: Complete Cases</h2>
<p>Take only complete cases:</p>
<pre class="r"><code>df_com &lt;- na.omit(df_num)</code></pre>
</div>
<div id="step-3-scaling" class="section level2">
<h2>Step 3: Scaling</h2>
<p>Scaling the dataset is important because different features may have different scales, and if they are not scaled, then the features with larger scales will have a greater influence on the clustering results.</p>
<p>Common scaling techniques used in clustering include standardization (subtracting the mean and dividing by the standard deviation), normalization (scaling the values to a specific range).</p>
<p><span class="math inline">\(X_{stand} = \frac{X - \mu_x}{\sigma_x}\)</span><br />
<span class="math inline">\(X_{norm} = \frac{X - X_{min}}{X_{max}-X_{min}}\)</span><br />
<span class="math inline">\(E[Y] = E[\Gamma&#39;X]=\Gamma&#39;E[X]=0\)</span></p>
<pre class="r"><code># standardization
df_scaled &lt;- data.frame(scale(df_com, center = TRUE, scale = TRUE))

# normalization
# df_scaled &lt;- scale(df_com, 
#                    center = apply(df_com, 2, min), 
#                    scale = apply(df_com, 2, function(x) {max(x) - min(x)}))  </code></pre>
</div>
<div id="step-4-optimal-number-of-clusters" class="section level2">
<h2>Step 4: Optimal Number of Clusters</h2>
<p>The optimal number of clusters depends on the specific dataset and the application. However, there are a few methods that can be used to help determine the optimal number of clusters in centroid-based clustering.</p>
<div id="elbow-method" class="section level3">
<h3>Elbow Method</h3>
<p>This method involves plotting the within-cluster sum of squares (WSS) as a function of the number of clusters. The WSS is a measure of how well the data points are clustered together. The elbow method works by looking for the point in the WSS curve where the rate of decrease starts to slow down. This point is often considered to be the optimal number of clusters.</p>
<pre class="r"><code>library(factoextra)
fviz_nbclust(df_scaled, kmeans, method = &quot;wss&quot;) +  
  geom_vline(xintercept = 2, linetype = 2) + # add line for better viz
  labs(subtitle = &quot;Elbow method&quot;) # add subtitle</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="silhouette-scorewidth" class="section level3">
<h3>Silhouette Score/Width</h3>
<p>The silhouette score is a measure of how well each data point is clustered with its own cluster compared to other clusters. It ranges from -1 to 1, with a value of 1 indicating that the data point is perfectly clustered with its own cluster, 0 indicating the data point is on the border between the two clusters and a value of -1 indicating that the data point is equally well-clustered with two or more clusters. The optimal number of clusters is often the number that maximizes the average silhouette score.</p>
<pre class="r"><code>fviz_nbclust(df_scaled, kmeans, method = &quot;silhouette&quot;) +  
  labs(subtitle = &quot;Silhouette method&quot;)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>To compute Silhouette information for a cluster:</p>
<pre class="r"><code>library(cluster)
km_res &lt;- kmeans(df_scaled, centers = 2)  # defining cluster

sil &lt;- silhouette(km_res$cluster, dist(df_scaled))
plot(sil)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>fviz_silhouette(sil) + theme_minimal()</code></pre>
<pre><code>#   cluster size ave.sil.width
# 1       1    8          0.48
# 2       2   12          0.26</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>A positive silhouette coefficient indicates that an observation is well-matched to its own cluster.</p>
</div>
<div id="gap-statistic-method" class="section level3">
<h3>Gap Statistic Method</h3>
<p>The gap statistic helps determine the optimal number of clusters by comparing the observed within-cluster dispersion with a reference distribution. The idea is to select the number of clusters that maximizes the gap statistic. A larger gap indicates a more significant deviation from randomness, suggesting a better-defined clustering structure. By comparing the gaps for different numbers of clusters, we can identify the number of clusters that provides the most distinct and meaningful clustering pattern.</p>
<pre class="r"><code>fviz_nbclust(df_scaled, kmeans, method = &quot;gap_stat&quot;) +
  labs(subtitle = &quot;Gap statistic method&quot;)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="combination-of-all" class="section level3">
<h3>Combination of All</h3>
<p>But the clever way is to try out different methods and see what majority returns. See <a href="https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#elbow-method">here</a></p>
<p>The <code>parameter</code> package comes to help in this case:</p>
<pre class="r"><code>library(parameters)
n_clust &lt;- n_clusters(df_scaled,
                      package = c(&quot;easystats&quot;, &quot;NbClust&quot;, &quot;mclust&quot;),
                      standardize = TRUE
                      )
print(n_clust)</code></pre>
<pre><code># # Method Agreement Procedure:
# 
# The choice of 2 clusters is supported by 7 (31.82%) methods out of 22 (Elbow, Silhouette, kl, Duda, Pseudot2, Ratkowsky, Mcclain).</code></pre>
<pre class="r"><code>plot(n_clust)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
</div>
<div id="step-5-hierarchical-clustering" class="section level2">
<h2>Step 5: Hierarchical Clustering</h2>
<div id="step-5.1-distance" class="section level3">
<h3>Step 5.1: Distance</h3>
<div id="numeric-data" class="section level4">
<h4>Numeric Data</h4>
<p>Use the function <code>dist()</code>:</p>
<pre class="r"><code>df_dist_euc &lt;- dist(df_scaled, method = &#39;euclidian&#39;)
# df_dist_max &lt;- dist(df_scaled, method = &#39;maximum&#39;)
# df_dist_man &lt;- dist(df_scaled, method = &#39;manhattan&#39;)
# df_dist_can &lt;- dist(df_scaled, method = &#39;canberra&#39;)
# df_dist_mink &lt;- dist(df_scaled, method = &#39;minkowski&#39;)</code></pre>
</div>
<div id="categorical-data" class="section level4">
<h4>Categorical Data</h4>
<pre class="r"><code># dist(dummified_data, method = &quot;binary&quot;)</code></pre>
</div>
<div id="gower-distance" class="section level4">
<h4>Gower Distance</h4>
<p>Gower distance is a similarity measure commonly used for clustering categorical and numerical mixed data. It takes into account the different types of variables and calculates the dissimilarity between observations based on their attribute values.</p>
<p>For categorical variables, Gower distance measures dissimilarity as the proportion of attributes where two observations differ. For numerical variables, it calculates the standardized absolute difference between the values. The Gower distance is then calculated as the weighted sum of the dissimilarities for all variables.</p>
<pre class="r"><code>df &lt;- dplyr::mutate_if(df, is.character, as.factor) # convert character to factor

df_dist_gower &lt;- cluster::daisy(df, metric = &quot;gower&quot;) # Dissimilarity Matrix Calculation</code></pre>
</div>
</div>
<div id="step-5.2-linkage-criteria" class="section level3">
<h3>Step 5.2: Linkage Criteria</h3>
<p>Linkage refers to the way in which clusters are merged together. There are several different linkage criteria that can be used, and the choice of linkage criterion can have a significant impact on the results of the clustering -</p>
<ul>
<li>Complete linkage: Maximum distance between two sets<br />
</li>
<li>Single linkage: Minimum distance between two sets<br />
</li>
<li>Average linkage: Average distance between two sets<br />
</li>
<li>Ward’s linkage: Minimize the total within cluster variance</li>
</ul>
<p><img src="index.en_insertimage_4.png" /></p>
<p><img src="index.en_insertimage_5.png" /></p>
<p>Using the Euclidean distance on the numeric data the following code finds the hierarchical clusters by different linkage methods:</p>
<pre class="r"><code>hclust_single &lt;- hclust(df_dist_euc, method = &quot;single&quot;)
hclust_complete &lt;- hclust(df_dist_euc, method = &quot;complete&quot;)
hclust_average &lt;- hclust(df_dist_euc, method = &quot;average&quot;)
hclust_centroid &lt;- hclust(df_dist_euc, method = &quot;centroid&quot;)
hclust_ward &lt;- hclust(df_dist_euc, method = &quot;ward.D2&quot;)</code></pre>
<p>Get the cluster information:</p>
<pre class="r"><code>cutree(hclust_ward, k = 2) # specify number of clusters</code></pre>
<pre><code># 202 102 112 151 198   4  55  70  98 135   7 183  43 189 140  51 146  25   2 179 
#   1   1   1   1   1   2   2   2   2   1   2   1   2   1   1   2   1   2   2   1</code></pre>
<pre class="r"><code>cutree(hclust_ward, h = 0.7) # specify height to cut</code></pre>
<pre><code># 202 102 112 151 198   4  55  70  98 135   7 183  43 189 140  51 146  25   2 179 
#   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20</code></pre>
<p>Summary by clusters:</p>
<pre class="r"><code>library(dplyr)
df_num %&gt;%
  mutate(cluster = cutree(hclust_ward, k = 2)) %&gt;% 
  group_by(cluster) %&gt;%
  summarise(across(everything(), list(Average = mean))) </code></pre>
<pre><code># # A tibble: 2 × 12
#   cluster Aroma_Average Flavor_Average Aftertaste_Average Acidity_Average
#     &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;
# 1       1          7.55           7.55               7.37            7.48
# 2       2          8.00           8.03               7.85            7.91
# # ℹ 7 more variables: Body_Average &lt;dbl&gt;, Balance_Average &lt;dbl&gt;,
# #   Overall_Average &lt;dbl&gt;, Total.Cup.Points_Average &lt;dbl&gt;,
# #   Moisture.Percentage_Average &lt;dbl&gt;, Category.One.Defects_Average &lt;dbl&gt;,
# #   Quakers_Average &lt;dbl&gt;</code></pre>
<p>Dendrogram will help to visualize as well as to determine the number of clusters to use or the height where to cut.</p>
</div>
<div id="step-5.3-dendrogram" class="section level3">
<h3>Step 5.3: Dendrogram</h3>
<p>Pass the hclust object to the plot function:</p>
<pre class="r"><code>plot(hclust_ward, hang = -1)
rect.hclust(hclust_ward, k = 2, border = &quot;blue&quot;)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Using dendextend package:</p>
<pre class="r"><code># install.packages(&quot;dendextend&quot;)
library(dendextend)

plot(color_branches(dend = hclust_ward, k = 2)) # specify k, can also use h to specify height</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>plot(color_branches(dend = hclust_ward, k = 2), 
     # parameters for nodes
     nodePar = list(col = 3:2, 
                    cex = c(1.5, 0.75), 
                    pch =  21:22,
                    bg =  c(&quot;light blue&quot;, &quot;pink&quot;),
                    lab.cex = 0.75, 
                    lab.col = &quot;tomato&quot;))</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
<pre class="r"><code># function to get color labels; Change color and cut based on number of clusters
colLab &lt;- function(n, labelColors = c(&quot;#036564&quot;, &quot;#EB6841&quot;), cut = 2) {
  clusMember = cutree(hclust_ward, cut)
  if (is.leaf(n)) {
    a &lt;- attributes(n)
    labCol &lt;- labelColors[clusMember[which(names(clusMember) == a$label)]]
    attr(n, &quot;nodePar&quot;) &lt;- c(a$nodePar, lab.col = labCol)
  }
  n
}

hclust_ward %&gt;% 
  color_branches(k = 2, col = c(&quot;#036564&quot;, &quot;#EB6841&quot;)) %&gt;% 
  dendrapply(colLab) %&gt;%  # applying to all nodes
  plot()  # finally plotting</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>library(&quot;ape&quot;)
plot(as.phylo(hclust_ward), type = &quot;unrooted&quot;, cex = 0.7, no.margin = T)</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
</div>
<div id="step-5-k-means-clustering" class="section level2">
<h2>Step 5: K-means Clustering</h2>
<p>Categorical data cannot be passed to the kmeans() function in R. The kmeans() function uses the Euclidean distance metric to calculate the distance between observations, and this metric is not defined for categorical data.</p>
<pre class="r"><code>km &lt;- kmeans(df_scaled, centers = 2) # centers specify number of clusters
km</code></pre>
<pre><code># K-means clustering with 2 clusters of sizes 12, 8
# 
# Cluster means:
#        Aroma     Flavor Aftertaste    Acidity       Body    Balance    Overall
# 1 -0.5530586 -0.5889311 -0.6267575 -0.5641766 -0.5577865 -0.6337761 -0.6825517
# 2  0.8295879  0.8833967  0.9401363  0.8462649  0.8366797  0.9506641  1.0238275
#   Total.Cup.Points Moisture.Percentage Category.One.Defects    Quakers
# 1       -0.6521312           0.3165048            0.2043483  0.2052711
# 2        0.9781969          -0.4747571           -0.3065225 -0.3079067
# 
# Clustering vector:
# 202 102 112 151 198   4  55  70  98 135   7 183  43 189 140  51 146  25   2 179 
#   1   1   1   1   1   2   2   2   1   1   2   1   2   1   1   2   1   2   2   1 
# 
# Within cluster sum of squares by cluster:
# [1] 88.09225 26.35215
#  (between_SS / total_SS =  45.2 %)
# 
# Available components:
# 
# [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
# [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<p>Visualizing clusters:</p>
<pre class="r"><code>fviz_cluster(km, df_scaled, ggtheme = theme_minimal())</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Visualizing clusters in terms of variables in the dataset:</p>
<pre class="r"><code>library(ggplot2)
df_km &lt;- mutate(df_num, cluster = km$cluster)
df_km %&gt;% ggplot(aes(x = Aroma, y = Aftertaste)) + 
  geom_point(aes(color = as.factor(cluster))) +
  labs(color = &quot;Clusters&quot;) + theme_minimal()</code></pre>
<p><img src="index.en_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Summary on clusters:</p>
<pre class="r"><code>df_km %&gt;%
  group_by(cluster) %&gt;%
  summarise(across(everything(), list(Average = mean))) </code></pre>
<pre><code># # A tibble: 2 × 12
#   cluster Aroma_Average Flavor_Average Aftertaste_Average Acidity_Average
#     &lt;int&gt;         &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;
# 1       1          7.56           7.57               7.40            7.50
# 2       2          8.04           8.06               7.88            7.93
# # ℹ 7 more variables: Body_Average &lt;dbl&gt;, Balance_Average &lt;dbl&gt;,
# #   Overall_Average &lt;dbl&gt;, Total.Cup.Points_Average &lt;dbl&gt;,
# #   Moisture.Percentage_Average &lt;dbl&gt;, Category.One.Defects_Average &lt;dbl&gt;,
# #   Quakers_Average &lt;dbl&gt;</code></pre>
<hr />
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ol style="list-style-type: decimal">
<li>ISLR<br />
</li>
<li><a href="https://uc-r.github.io/kmeans_clustering" class="uri">https://uc-r.github.io/kmeans_clustering</a><br />
</li>
<li><a href="https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#elbow-method" class="uri">https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/#elbow-method</a><br />
</li>
<li><a href="https://www.gastonsanchez.com/visually-enforced/how-to/2012/10/03/Dendrograms/" class="uri">https://www.gastonsanchez.com/visually-enforced/how-to/2012/10/03/Dendrograms/</a></li>
</ol>
</div>
